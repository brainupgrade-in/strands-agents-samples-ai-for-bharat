{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Built-In Evaluators - Measuring Agent Performance with Strands Evals\n",
    "\n",
    "This tutorial introduces the complete toolkit of built-in evaluators provided by Strands Evals. You'll learn how to measure different aspects of agent performance using standardized evaluation metrics, from response quality to tool selection accuracy.\n",
    "\n",
    "### What You'll Learn\n",
    "- Understand the purpose of each built-in evaluator\n",
    "- Apply OutputEvaluator with domain-specific rubrics\n",
    "- Use trace-based evaluators (HelpfulnessEvaluator, GoalSuccessRateEvaluator, ToolSelectionAccuracyEvaluator, ToolParameterAccuracyEvaluator)\n",
    "- Analyze agent reasoning with TrajectoryEvaluator\n",
    "- Compare evaluation results across different metrics\n",
    "\n",
    "### Tutorial Details\n",
    "\n",
    "| Information         | Details                                                                       |\n",
    "|:--------------------|:------------------------------------------------------------------------------|\n",
    "| Tutorial type       | Beginner - Introduction to built-in evaluation metrics                        |\n",
    "| Tutorial components | Recipe Bot agent, 6 built-in evaluators, results visualization               |\n",
    "| Tutorial vertical   | Agent Evaluation                                                              |\n",
    "| Example complexity  | Easy                                                                          |\n",
    "| SDK used            | Strands Agents, Strands Evals                                                 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Built-In Evaluators\n",
    "\n",
    "Evaluating agent performance is complex because agents operate across multiple dimensions\u2014they generate responses, complete tasks, and use tools to achieve goals. A single metric can't capture all aspects of agent behavior, which is why Strands Evals provides six specialized built-in evaluators.\n",
    "\n",
    "| Evaluator | Type | Use When | Measures | Requirements |\n",
    "|:----------|:-----|:---------|:---------|:-------------|\n",
    "| **OutputEvaluator** | Output-based | Verify correct, complete answers | Correctness, completeness, relevance via custom rubrics | None |\n",
    "| **HelpfulnessEvaluator** | Trace-based | Ensure agent is genuinely useful | Practical value, clarity, actionability (7-point scale) | OpenTelemetry and Session mapping |\n",
    "| **GoalSuccessRateEvaluator** | Trace-based | Track task completion rates | Binary success/failure against defined goals | OpenTelemetry and Session mapping |\n",
    "| **ToolSelectionAccuracyEvaluator** | Trace-based | Verify proper tool selection | Whether agent chose the right tools | OpenTelemetry and Session mapping |\n",
    "| **ToolParameterAccuracyEvaluator** | Trace-based | Validate tool parameter usage | Correctness of tool parameter values | OpenTelemetry and Session mapping |\n",
    "| **TrajectoryEvaluator** | Trajectory-based | Understand agent reasoning | Quality of reasoning steps and action sequences | Trajectory extractor |\n",
    "\n",
    "#### Important API Note\n",
    "\n",
    "**ONE Evaluator Per Dataset**: Each Dataset accepts exactly ONE evaluator object. To demonstrate multiple evaluators, we run separate evaluation rounds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Setup\n",
    "\n",
    "Configure AWS region and model settings for this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# AWS Configuration\n",
    "session = boto3.Session()\n",
    "AWS_REGION = session.region_name or 'us-east-1'\n",
    "DEFAULT_MODEL = 'us.anthropic.claude-3-7-sonnet-20250219-v1:0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup and Imports\n",
    "\n",
    "Import all necessary libraries for agent creation and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import json\n",
    "from typing import List, Dict\n",
    "\n",
    "# Strands imports\n",
    "from strands import Agent, tool\n",
    "\n",
    "# Strands Evals imports\n",
    "from strands_evals import Dataset, Case\n",
    "from strands_evals.evaluators import (\n",
    "    OutputEvaluator,\n",
    "    HelpfulnessEvaluator,\n",
    "    GoalSuccessRateEvaluator,\n",
    "    ToolSelectionAccuracyEvaluator,\n",
    "    ToolParameterAccuracyEvaluator,\n",
    "    TrajectoryEvaluator\n",
    ")\n",
    "from strands_evals.extractors import tools_use_extractor\n",
    "\n",
    "# Display utilities\n",
    "from IPython.display import Markdown, display\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recipe Bot Agent\n",
    "\n",
    "We'll use a Recipe Bot agent to demonstrate built-in evaluators. This agent helps users find recipes and answers cooking questions using web search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ddgs import DDGS\n",
    "from ddgs.exceptions import DDGSException, RatelimitException\n",
    "import time\n",
    "\n",
    "# Define a websearch tool\n",
    "@tool\n",
    "def websearch(\n",
    "    keywords: str, region: str = \"us-en\", max_results: int | None = None\n",
    ") -> str:\n",
    "    \"\"\"Search the web to get updated information.\n",
    "    Args:\n",
    "        keywords (str): The search query keywords.\n",
    "        region (str): The search region: wt-wt, us-en, uk-en, ru-ru, etc..\n",
    "        max_results (int | None): The maximum number of results to return.\n",
    "    Returns:\n",
    "        List of dictionaries with search results.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        time.sleep(15)\n",
    "        results = DDGS().text(keywords, region=region, max_results=max_results)\n",
    "        return results if results else \"No results found.\"\n",
    "    except RatelimitException:\n",
    "        return \"RatelimitException: Please try again after a short delay.\"\n",
    "    except DDGSException as d:\n",
    "        return f\"DuckDuckGoSearchException: {d}\"\n",
    "    except Exception as e:\n",
    "        return f\"Exception: {e}\"\n",
    "\n",
    "\n",
    "# System prompt for Recipe Bot\n",
    "RECIPE_BOT_SYSTEM_PROMPT = \"\"\"You are RecipeBot, a helpful cooking assistant.\n",
    "Help users find recipes based on ingredients and answer cooking questions.\n",
    "Use the websearch tool to find recipes when users mention ingredients or to look up cooking information.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Agent\n",
    "\n",
    "Before evaluating, let's verify the agent works correctly with a simple test query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a test agent instance\n",
    "test_agent = Agent(\n",
    "    system_prompt=RECIPE_BOT_SYSTEM_PROMPT,\n",
    "    tools=[websearch],\n",
    "    model=DEFAULT_MODEL\n",
    ")\n",
    "\n",
    "# Test with a simple query\n",
    "test_query = \"What can I make with chicken and tomatoes?\"\n",
    "test_response = test_agent(test_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Test Cases\n",
    "\n",
    "We'll create test cases with domain-specific expectations for Recipe Bot evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test cases for evaluation\n",
    "test_cases = [\n",
    "    Case(\n",
    "        name=\"Recipe Search - Simple Ingredients\",\n",
    "        input=\"I have chicken and broccoli. What can I cook?\",\n",
    "        expected_output=\"A helpful response with recipe suggestions that include chicken and broccoli, with cooking instructions or search results.\",\n",
    "        metadata={\n",
    "            \"goal\": \"Find recipes using specified ingredients\",\n",
    "            \"expected_tools\": [\"websearch\"],\n",
    "            \"expected_tool_params\": {\n",
    "                \"websearch\": {\n",
    "                    \"keywords\": [\"chicken\", \"broccoli\", \"recipe\"]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ),\n",
    "    Case(\n",
    "        name=\"Cooking Technique Question\",\n",
    "        input=\"How do I properly sear a steak?\",\n",
    "        expected_output=\"Clear instructions on steak searing technique, including temperature, timing, and tips for achieving a good sear.\",\n",
    "        metadata={\n",
    "            \"goal\": \"Learn proper steak searing technique\",\n",
    "            \"expected_tools\": [\"websearch\"],\n",
    "            \"expected_tool_params\": {\n",
    "                \"websearch\": {\n",
    "                    \"keywords\": [\"sear\", \"steak\"]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ),\n",
    "    Case(\n",
    "        name=\"Dietary Restriction Recipe\",\n",
    "        input=\"Can you find me a vegan pasta recipe?\",\n",
    "        expected_output=\"One or more vegan pasta recipes with ingredients and preparation steps.\",\n",
    "        metadata={\n",
    "            \"goal\": \"Find vegan pasta recipes\",\n",
    "            \"expected_tools\": [\"websearch\"],\n",
    "            \"expected_tool_params\": {\n",
    "                \"websearch\": {\n",
    "                    \"keywords\": [\"vegan\", \"pasta\", \"recipe\"]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenTelemetry Setup\n",
    "\n",
    "Trace-based evaluators (HelpfulnessEvaluator, GoalSuccessRateEvaluator, ToolSelectionAccuracyEvaluator, ToolParameterAccuracyEvaluator) require OpenTelemetry setup to capture agent execution traces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from strands_evals.telemetry import StrandsEvalsTelemetry\n",
    "from strands_evals.mappers import StrandsInMemorySessionMapper\n",
    "\n",
    "# Setup telemetry - CORRECT WAY per README\n",
    "telemetry = StrandsEvalsTelemetry().setup_in_memory_exporter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Round 1: OutputEvaluator\n",
    "\n",
    "OutputEvaluator assesses response quality using domain-specific rubrics. For Recipe Bot, we check for ingredient lists, cooking instructions, and timing information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create OutputEvaluator with domain-specific rubric\n",
    "output_evaluator = OutputEvaluator(\n",
    "    rubric=\"\"\"Recipe responses should include:\n",
    "    1. Clear ingredient list with quantities (0-0.3 points)\n",
    "    2. Step-by-step cooking instructions (0-0.4 points)\n",
    "    3. Cooking time/temperature if applicable (0-0.3 points)\n",
    "    Score proportionally based on completeness.\"\"\"\n",
    ")\n",
    "\n",
    "# Create dataset with OutputEvaluator\n",
    "output_dataset = Dataset[str, str](cases=test_cases, evaluator=output_evaluator)\n",
    "\n",
    "# Simple task function (no OTEL needed for OutputEvaluator)\n",
    "def simple_task(case: Case) -> str:\n",
    "    agent = Agent(\n",
    "        system_prompt=RECIPE_BOT_SYSTEM_PROMPT,\n",
    "        tools=[websearch],\n",
    "        model=DEFAULT_MODEL\n",
    "    )\n",
    "    return str(agent(case.input))\n",
    "\n",
    "# Run evaluation\n",
    "output_report = output_dataset.run_evaluations(simple_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display OutputEvaluator results\n",
    "output_report.run_display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Round 2: HelpfulnessEvaluator\n",
    "\n",
    "HelpfulnessEvaluator measures how useful the agent's response is to users on a 7-point scale. This evaluator requires OpenTelemetry Session data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create HelpfulnessEvaluator\n",
    "helpfulness_evaluator = HelpfulnessEvaluator()\n",
    "\n",
    "# Create dataset\n",
    "helpfulness_dataset = Dataset[str, str](cases=test_cases, evaluator=helpfulness_evaluator)\n",
    "\n",
    "# Task function with OTEL support\n",
    "import uuid\n",
    "\n",
    "def trace_task(case: Case) -> dict:\n",
    "    telemetry.in_memory_exporter.clear()\n",
    "    session_id = str(uuid.uuid4())  # Generate unique session ID\n",
    "    agent = Agent(\n",
    "        system_prompt=RECIPE_BOT_SYSTEM_PROMPT,\n",
    "        tools=[websearch],\n",
    "        model=DEFAULT_MODEL,\n",
    "        trace_attributes={\"session.id\": session_id},\n",
    "        callback_handler=None\n",
    "    )\n",
    "    response = agent(case.input)\n",
    "    \n",
    "    # Force flush all spans to ensure they're captured\n",
    "    telemetry.tracer_provider.force_flush()\n",
    "\n",
    "    # Map spans to Session\n",
    "    finished_spans = telemetry.in_memory_exporter.get_finished_spans()\n",
    "    mapper = StrandsInMemorySessionMapper()\n",
    "    session = mapper.map_to_session(finished_spans, session_id=session_id)\n",
    "\n",
    "    return {\"output\": str(response), \"trajectory\": session}\n",
    "\n",
    "# Run evaluation\n",
    "helpfulness_report = helpfulness_dataset.run_evaluations(trace_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helpfulness_report.run_display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Round 3: GoalSuccessRateEvaluator\n",
    "\n",
    "GoalSuccessRateEvaluator determines if the agent successfully completed the user's stated goal (binary success/failure)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create GoalSuccessRateEvaluator\n",
    "goal_evaluator = GoalSuccessRateEvaluator()\n",
    "\n",
    "# Create dataset\n",
    "goal_dataset = Dataset[str, str](cases=test_cases, evaluator=goal_evaluator)\n",
    "\n",
    "# Run evaluation (reuse trace_task function)\n",
    "goal_report = goal_dataset.run_evaluations(trace_task)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display GoalSuccessRateEvaluator results\n",
    "goal_report.run_display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Round 4: ToolSelectionAccuracyEvaluator\n",
    "\n",
    "ToolSelectionAccuracyEvaluator validates that the agent selected the correct tools for the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ToolSelectionAccuracyEvaluator\n",
    "tool_selection_evaluator = ToolSelectionAccuracyEvaluator()\n",
    "\n",
    "# Create dataset\n",
    "tool_selection_dataset = Dataset[str, str](cases=test_cases, evaluator=tool_selection_evaluator)\n",
    "\n",
    "# Run evaluation\n",
    "tool_selection_report = tool_selection_dataset.run_evaluations(trace_task)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display ToolSelectionAccuracyEvaluator results\n",
    "tool_selection_report.run_display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Round 5: ToolParameterAccuracyEvaluator\n",
    "\n",
    "ToolParameterAccuracyEvaluator checks if the agent used tool parameters correctly (e.g., proper search keywords)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ToolParameterAccuracyEvaluator\n",
    "tool_parameter_evaluator = ToolParameterAccuracyEvaluator()\n",
    "\n",
    "# Create dataset\n",
    "tool_parameter_dataset = Dataset[str, str](cases=test_cases, evaluator=tool_parameter_evaluator)\n",
    "\n",
    "# Run evaluation\n",
    "tool_parameter_report = tool_parameter_dataset.run_evaluations(trace_task)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display ToolParameterAccuracyEvaluator results\n",
    "tool_parameter_report.run_display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Round 6: TrajectoryEvaluator\n",
    "\n",
    "TrajectoryEvaluator analyzes the sequence of actions (tool calls) the agent took to reach its conclusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TrajectoryEvaluator with domain-specific rubric\n",
    "trajectory_evaluator = TrajectoryEvaluator(\n",
    "    rubric=\"\"\"Agent should:\n",
    "    1. Understand user's ingredients/dietary needs\n",
    "    2. Search web with relevant recipe keywords\n",
    "    3. Synthesize results into actionable recommendations\n",
    "    Score: 1.0 if all steps present and logical, 0.5 if partially correct, 0.0 if flawed.\"\"\"\n",
    ")\n",
    "\n",
    "# Create dataset (use single test case for demonstration)\n",
    "trajectory_dataset = Dataset[str, str](cases=[test_cases[0]], evaluator=trajectory_evaluator)\n",
    "\n",
    "# Task function with trajectory extraction\n",
    "def trajectory_task(case: Case) -> dict:\n",
    "    agent = Agent(\n",
    "        system_prompt=RECIPE_BOT_SYSTEM_PROMPT,\n",
    "        tools=[websearch],\n",
    "        model=DEFAULT_MODEL\n",
    "    )\n",
    "    response = agent(case.input)\n",
    "\n",
    "    # Update trajectory description\n",
    "    trajectory_evaluator.update_trajectory_description(\n",
    "        tools_use_extractor.extract_tools_description(agent)\n",
    "    )\n",
    "\n",
    "    # Extract trajectory from agent messages\n",
    "    trajectory = tools_use_extractor.extract_agent_tools_used_from_messages(agent.messages)\n",
    "    return {\"output\": str(response), \"trajectory\": trajectory}\n",
    "\n",
    "# Run evaluation\n",
    "trajectory_report = trajectory_dataset.run_evaluations(trajectory_task)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display TrajectoryEvaluator results\n",
    "trajectory_report.run_display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary: Comparing All Evaluators\n",
    "\n",
    "Let's create a summary table comparing results from all six evaluators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary comparison table\n",
    "summary_data = {\n",
    "    \"Evaluator\": [\n",
    "        \"OutputEvaluator\",\n",
    "        \"HelpfulnessEvaluator\",\n",
    "        \"GoalSuccessRateEvaluator\",\n",
    "        \"ToolSelectionAccuracyEvaluator\",\n",
    "        \"ToolParameterAccuracyEvaluator\",\n",
    "        \"TrajectoryEvaluator\"\n",
    "    ],\n",
    "    \"Overall Score\": [\n",
    "        f\"{output_report.overall_score:.2f}\",\n",
    "        f\"{helpfulness_report.overall_score:.2f}\",\n",
    "        f\"{goal_report.overall_score:.2f}\",\n",
    "        f\"{tool_selection_report.overall_score:.2f}\",\n",
    "        f\"{tool_parameter_report.overall_score:.2f}\",\n",
    "        f\"{trajectory_report.overall_score:.2f}\"\n",
    "    ],\n",
    "    \"Type\": [\n",
    "        \"Output-based\",\n",
    "        \"Trace-based\",\n",
    "        \"Trace-based\",\n",
    "        \"Trace-based\",\n",
    "        \"Trace-based\",\n",
    "        \"Trajectory-based\"\n",
    "    ],\n",
    "    \"What It Measures\": [\n",
    "        \"Response quality (ingredients, instructions, timing)\",\n",
    "        \"User satisfaction (7-point scale)\",\n",
    "        \"Goal completion (binary success/failure)\",\n",
    "        \"Correct tool selection\",\n",
    "        \"Correct tool parameters (keywords)\",\n",
    "        \"Action sequence quality\"\n",
    "    ],\n",
    "    \"Requirements\": [\n",
    "        \"None\",\n",
    "        \"OpenTelemetry + Session\",\n",
    "        \"OpenTelemetry + Session\",\n",
    "        \"OpenTelemetry + Session\",\n",
    "        \"OpenTelemetry + Session\",\n",
    "        \"Trajectory extractor\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "display(Markdown(\"### Built-In Evaluator Comparison\"))\n",
    "display(summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Production Recommendations\n",
    "\n",
    "For comprehensive agent evaluation:\n",
    "1. Start with OutputEvaluator using domain-specific rubrics\n",
    "2. Add HelpfulnessEvaluator and GoalSuccessRateEvaluator for user-centric metrics\n",
    "3. Use tool evaluators if your agent has multiple tools or complex tool usage\n",
    "4. Apply TrajectoryEvaluator for debugging and reasoning analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "You've successfully learned how to use built-in evaluators provided by Strands Evals.\n",
    "\n",
    "In the next tutorial, you'll learn how to create custom evaluators for specialized evaluation needs beyond what the built-in evaluators provide."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai-on-aws",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}