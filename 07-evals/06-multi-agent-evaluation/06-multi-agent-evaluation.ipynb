{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Agent Evaluation - Evaluating Collaborative Agent Systems\n",
    "\n",
    "This tutorial demonstrates how to evaluate multi-agent systems where multiple specialized agents collaborate to complete complex tasks. You'll learn to assess individual agent performance, collective system outcomes, and the quality of agent coordination and handoffs.\n",
    "\n",
    "### What You'll Learn\n",
    "- Understand multi-agent architectures (agent-as-tool pattern)\n",
    "- Evaluate individual agent performance separately\n",
    "- Assess collective system performance as a whole\n",
    "- Measure coordination quality using InteractionsEvaluator\n",
    "- Analyze agent handoffs and collaboration patterns\n",
    "- Generate comprehensive multi-agent evaluation reports\n",
    "\n",
    "### Tutorial Details\n",
    "\n",
    "| Information         | Details                                                                       |\n",
    "|:--------------------|:------------------------------------------------------------------------------|\n",
    "| Tutorial type       | Advanced - Evaluating complex multi-agent collaborative systems               |\n",
    "| Tutorial components | Multi-agent orchestrator, InteractionsEvaluator, coordination analysis        |\n",
    "| Tutorial vertical   | Agent Evaluation                                                              |\n",
    "| Example complexity  | Advanced                                                                      |\n",
    "| SDK used            | Strands Agents, Strands Evals                                                 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Multi-Agent Systems\n",
    "\n",
    "A **multi-agent system** consists of multiple specialized AI agents that collaborate to solve complex problems, coordinated by an orchestrator.\n",
    "\n",
    "#### Agent-as-Tool Pattern\n",
    "\n",
    "| Component | Role |\n",
    "|:----------|:-----|\n",
    "| Orchestrator Agent | Routes user queries to appropriate specialists |\n",
    "| Specialist Agents | Wrapped as callable tools with focused expertise |\n",
    "| Coordination | Orchestrator decides which specialist(s) to invoke |\n",
    "\n",
    "#### Architecture\n",
    "\n",
    "```\n",
    "User Query \u2192 Orchestrator Agent\n",
    "                 \u251c\u2500\u2500> Technical Support Agent\n",
    "                 \u251c\u2500\u2500> Billing Support Agent\n",
    "                 \u251c\u2500\u2500> Product Info Agent\n",
    "                 \u2514\u2500\u2500> Returns & Exchanges Agent\n",
    "```\n",
    "\n",
    "#### Why Evaluate Multi-Agent Systems?\n",
    "\n",
    "| Concern | Impact |\n",
    "|:--------|:-------|\n",
    "| Individual agent quality | Affects overall system performance |\n",
    "| Poor coordination | Leads to incorrect or inefficient outcomes |\n",
    "| Handoff quality | Impacts user experience |\n",
    "| Emergent behaviors | May differ from individual agent capabilities |\n",
    "\n",
    "#### Three Evaluation Dimensions\n",
    "\n",
    "| Dimension | Focus | Metrics |\n",
    "|:----------|:------|:--------|\n",
    "| Individual Performance | Each specialist agent's task competence | Tool selection, response quality, completeness |\n",
    "| Collective Performance | Overall system output | Final quality, task completion, user satisfaction |\n",
    "| Coordination Quality | Agent collaboration | Routing accuracy, information passing, handoff smoothness |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Setup\n",
    "\n",
    "Configure AWS region and model settings for this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# AWS Configuration (inline - no config.py)\n",
    "session = boto3.Session()\n",
    "AWS_REGION = session.region_name or 'us-east-1'\n",
    "DEFAULT_MODEL = 'us.anthropic.claude-3-7-sonnet-20250219-v1:0'\n",
    "\n",
    "print(f\"AWS Region: {AWS_REGION}\")\n",
    "print(f\"Model: {DEFAULT_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup and Imports\n",
    "\n",
    "Import all necessary libraries for multi-agent creation, evaluation, and coordination analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Dict, List, Any\n",
    "import uuid\n",
    "\n",
    "# Strands imports\n",
    "from strands import Agent, tool\n",
    "\n",
    "# Strands Evals imports\n",
    "from strands_evals import Dataset, Case\n",
    "from strands_evals.evaluators import OutputEvaluator, ToolSelectionAccuracyEvaluator, InteractionsEvaluator\n",
    "from strands_evals.extractors import tools_use_extractor\n",
    "from strands_evals.types import Interaction\n",
    "from strands_evals.telemetry import StrandsEvalsTelemetry\n",
    "from strands_evals.mappers import StrandsInMemorySessionMapper\n",
    "\n",
    "# Display utilities\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Bypass tool consent for automated execution\n",
    "os.environ[\"BYPASS_TOOL_CONSENT\"] = \"true\"\n",
    "\n",
    "# Setup telemetry for trace-based evaluators\n",
    "telemetry = StrandsEvalsTelemetry().setup_in_memory_exporter()\n",
    "\n",
    "print(\"All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Agent Customer Support System\n",
    "\n",
    "We'll build a customer support system with an orchestrator and four specialized agents. This demonstrates the agent-as-tool pattern.\n",
    "\n",
    "#### Agent Code\n",
    "\n",
    "Multi-agent code adapted from: /strands-samples/01-tutorials/02-multi-agent-systems/01-agent-as-tool/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Create Fake Database\n",
    "\n",
    "Create a simple database to simulate customer data, products, and orders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fake database for customer support\n",
    "FAKE_DATABASE = {\n",
    "    \"customers\": {\n",
    "        \"user123\": {\n",
    "            \"name\": \"John Doe\",\n",
    "            \"email\": \"john@example.com\",\n",
    "            \"subscription\": \"Pro\",\n",
    "            \"last_payment\": \"2024-01-15\",\n",
    "        },\n",
    "        \"user456\": {\n",
    "            \"name\": \"Jane Smith\",\n",
    "            \"email\": \"jane@example.com\",\n",
    "            \"subscription\": \"Basic\",\n",
    "            \"last_payment\": \"2024-01-10\",\n",
    "        },\n",
    "    },\n",
    "    \"products\": {\n",
    "        \"pro_plan\": {\n",
    "            \"name\": \"Pro Plan\",\n",
    "            \"price\": 29.99,\n",
    "            \"features\": [\"Advanced analytics\", \"Priority support\", \"Custom integrations\"],\n",
    "        },\n",
    "        \"basic_plan\": {\n",
    "            \"name\": \"Basic Plan\",\n",
    "            \"price\": 9.99,\n",
    "            \"features\": [\"Basic analytics\", \"Email support\"]\n",
    "        },\n",
    "    },\n",
    "    \"orders\": {\n",
    "        \"order789\": {\n",
    "            \"customer_id\": \"user123\",\n",
    "            \"product\": \"pro_plan\",\n",
    "            \"status\": \"shipped\",\n",
    "            \"date\": \"2024-01-20\"\n",
    "        },\n",
    "        \"order101\": {\n",
    "            \"customer_id\": \"user456\",\n",
    "            \"product\": \"basic_plan\",\n",
    "            \"status\": \"processing\",\n",
    "            \"date\": \"2024-01-22\"\n",
    "        },\n",
    "    },\n",
    "    \"tickets\": [],\n",
    "}\n",
    "\n",
    "print(\"Fake database created with customers, products, orders, and tickets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Define Database Tools\n",
    "\n",
    "Create tools for looking up customer, product, and order information, and creating support tickets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def lookup_customer(customer_id: str) -> str:\n",
    "    \"\"\"Look up customer information by ID.\"\"\"\n",
    "    customer = FAKE_DATABASE[\"customers\"].get(customer_id)\n",
    "    if customer:\n",
    "        return f\"Customer {customer_id}: {customer['name']} ({customer['email']}) - {customer['subscription']} plan, last payment: {customer['last_payment']}\"\n",
    "    return f\"Customer {customer_id} not found\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def lookup_product(product_id: str) -> str:\n",
    "    \"\"\"Look up product information by ID.\"\"\"\n",
    "    product = FAKE_DATABASE[\"products\"].get(product_id)\n",
    "    if product:\n",
    "        features = \", \".join(product[\"features\"])\n",
    "        return f\"Product {product_id}: {product['name']} - ${product['price']}/month. Features: {features}\"\n",
    "    return f\"Product {product_id} not found\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def lookup_order(order_id: str) -> str:\n",
    "    \"\"\"Look up order information by ID.\"\"\"\n",
    "    order = FAKE_DATABASE[\"orders\"].get(order_id)\n",
    "    if order:\n",
    "        return f\"Order {order_id}: Customer {order['customer_id']}, Product {order['product']}, Status: {order['status']}, Date: {order['date']}\"\n",
    "    return f\"Order {order_id} not found\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def create_ticket(customer_id: str, issue_type: str, description: str) -> str:\n",
    "    \"\"\"Create a support ticket.\"\"\"\n",
    "    import datetime\n",
    "    \n",
    "    ticket_id = f\"ticket{len(FAKE_DATABASE['tickets']) + 1}\"\n",
    "    ticket = {\n",
    "        \"id\": ticket_id,\n",
    "        \"customer_id\": customer_id,\n",
    "        \"issue_type\": issue_type,\n",
    "        \"description\": description,\n",
    "        \"status\": \"open\",\n",
    "        \"created\": datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    }\n",
    "    FAKE_DATABASE[\"tickets\"].append(ticket)\n",
    "    return f\"Created {ticket_id} for customer {customer_id}: {issue_type} - {description}\"\n",
    "\n",
    "\n",
    "print(\"Database tools defined: lookup_customer, lookup_product, lookup_order, create_ticket\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Define Specialist Agents as Tools\n",
    "\n",
    "Create four specialized support agents, each wrapped as a tool that can be called by the orchestrator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def technical_support(query: str) -> str:\n",
    "    \"\"\"Handle technical issues, bugs, and troubleshooting.\"\"\"\n",
    "    agent = Agent(\n",
    "        name=\"technical_support\",\n",
    "        model=DEFAULT_MODEL,\n",
    "        system_prompt=\"You are a technical support specialist. Help users troubleshoot technical issues, bugs, and product functionality problems. Use available tools to look up customer info and create tickets.\",\n",
    "        tools=[lookup_customer, create_ticket],\n",
    "    )\n",
    "    return str(agent(query))\n",
    "\n",
    "\n",
    "@tool\n",
    "def billing_support(query: str) -> str:\n",
    "    \"\"\"Handle billing, payments, and account issues.\"\"\"\n",
    "    agent = Agent(\n",
    "        name=\"billing_support\",\n",
    "        model=DEFAULT_MODEL,\n",
    "        system_prompt=\"You are a billing specialist. Help with payment issues, subscription questions, refunds, and account billing problems. Use tools to look up customer information.\",\n",
    "        tools=[lookup_customer],\n",
    "    )\n",
    "    return str(agent(query))\n",
    "\n",
    "\n",
    "@tool\n",
    "def product_info(query: str) -> str:\n",
    "    \"\"\"Provide product information and feature explanations.\"\"\"\n",
    "    agent = Agent(\n",
    "        name=\"product_info\",\n",
    "        model=DEFAULT_MODEL,\n",
    "        system_prompt=\"You are a product specialist. Explain product features, capabilities, and help users understand how to use products effectively. Use tools to look up product details.\",\n",
    "        tools=[lookup_product],\n",
    "    )\n",
    "    return str(agent(query))\n",
    "\n",
    "\n",
    "@tool\n",
    "def returns_exchanges(query: str) -> str:\n",
    "    \"\"\"Handle returns, exchanges, and any order issues.\"\"\"\n",
    "    agent = Agent(\n",
    "        name=\"returns_exchanges\",\n",
    "        model=DEFAULT_MODEL,\n",
    "        system_prompt=\"You are a returns specialist. Help with product returns, exchanges, order modifications, and shipping issues. Use tools to look up order and customer information.\",\n",
    "        tools=[lookup_order, lookup_customer],\n",
    "    )\n",
    "    return str(agent(query))\n",
    "\n",
    "\n",
    "print(\"Specialist agents created: technical_support, billing_support, product_info, returns_exchanges\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Create Orchestrator Agent\n",
    "\n",
    "The orchestrator routes customer queries to the appropriate specialist agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define orchestrator system prompt with clear routing guidance\n",
    "ORCHESTRATOR_PROMPT = \"\"\"\n",
    "You are a customer support router. Direct queries to the appropriate specialist:\n",
    "- Technical issues, bugs, errors \u2192 technical_support\n",
    "- Billing, payments, subscriptions \u2192 billing_support  \n",
    "- Product questions, features \u2192 product_info\n",
    "- Returns, exchanges, orders \u2192 returns_exchanges\n",
    "- Simple greetings \u2192 answer directly\n",
    "\"\"\"\n",
    "\n",
    "# Create the orchestrator agent\n",
    "orchestrator = Agent(\n",
    "    name=\"orchestrator\",\n",
    "    model=DEFAULT_MODEL,\n",
    "    system_prompt=ORCHESTRATOR_PROMPT,\n",
    "    tools=[technical_support, billing_support, product_info, returns_exchanges],\n",
    ")\n",
    "\n",
    "print(\"Orchestrator agent created\")\n",
    "print(\"\\nMulti-agent customer support system ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Multi-Agent System\n",
    "\n",
    "Before evaluating, let's test the system to see how the orchestrator routes queries to specialists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test query\n",
    "test_query = \"My app keeps crashing when I try to login. My customer ID is user123.\"\n",
    "\n",
    "print(f\"Test Query: {test_query}\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Execute the query\n",
    "response = orchestrator(test_query)\n",
    "\n",
    "print(f\"\\nResponse: {response}\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Show which tools were called\n",
    "print(\"\\nTools called during execution:\")\n",
    "for msg in orchestrator.messages:\n",
    "    if hasattr(msg, 'content'):\n",
    "        for content_block in msg.content:\n",
    "            if hasattr(content_block, 'tool_use_id'):\n",
    "                print(f\"  - {content_block.name if hasattr(content_block, 'name') else 'tool'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimension 1: Individual Agent Performance\n",
    "\n",
    "First, we'll evaluate how well each specialist agent performs its specific task when called directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Test Cases for Individual Agents\n",
    "\n",
    "Define test cases targeting each specialist agent's domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cases for individual agent evaluation\n",
    "individual_test_cases = [\n",
    "    Case(\n",
    "        name=\"Technical Support - App Crash\",\n",
    "        input=\"My app keeps crashing when I try to login for user123\",\n",
    "        expected_output=\"Technical support should look up customer info and create a ticket for the crash issue\"\n",
    "    ),\n",
    "    Case(\n",
    "        name=\"Billing Support - Double Charge\",\n",
    "        input=\"I was charged twice this month, my customer ID is user456\",\n",
    "        expected_output=\"Billing support should look up customer payment history and address the double charge concern\"\n",
    "    ),\n",
    "    Case(\n",
    "        name=\"Product Info - Features\",\n",
    "        input=\"What features are included in the pro_plan?\",\n",
    "        expected_output=\"Product specialist should look up pro_plan details and explain its features\"\n",
    "    ),\n",
    "    Case(\n",
    "        name=\"Returns - Order Status\",\n",
    "        input=\"I want to check the status of order789\",\n",
    "        expected_output=\"Returns specialist should look up the order and provide status information\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "print(f\"Created {len(individual_test_cases)} test cases for individual agent evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Individual Agent Evaluators\n",
    "\n",
    "Set up evaluators to assess output quality and tool selection for each specialist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output evaluator for response quality\n",
    "output_evaluator = OutputEvaluator(\n",
    "    rubric=\"Evaluate the quality and accuracy of the specialist agent's response for their specific domain.\",\n",
    "    model=DEFAULT_MODEL\n",
    ")\n",
    "\n",
    "# Tool selection evaluator for correct tool usage\n",
    "tool_selection_evaluator = ToolSelectionAccuracyEvaluator(\n",
    "    system_prompt=\"Score 1.0 if the agent uses the appropriate tools for the query (e.g., lookup_customer for billing, lookup_product for product info, create_ticket for technical issues). Score 0.5 if partially correct. Score 0.0 if wrong tools used.\",\n",
    "    model=DEFAULT_MODEL\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Specialist Agents Individually\n",
    "\n",
    "Run evaluation on each specialist agent when called directly (bypassing the orchestrator)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define direct agent tasks (bypassing orchestrator) with telemetry capture\n",
    "def technical_support_task(case: Case) -> dict:\n",
    "    \"\"\"Call technical support agent directly with telemetry capture.\"\"\"\n",
    "    telemetry.in_memory_exporter.clear()\n",
    "    session_id = str(uuid.uuid4())\n",
    "    agent = Agent(\n",
    "        name=\"technical_support\",\n",
    "        model=DEFAULT_MODEL,\n",
    "        system_prompt=\"You are a technical support specialist. Help users troubleshoot technical issues, bugs, and product functionality problems. Use available tools to look up customer info and create tickets.\",\n",
    "        tools=[lookup_customer, create_ticket],\n",
    "        trace_attributes={\"session.id\": session_id},\n",
    "        callback_handler=None\n",
    "    )\n",
    "    response = agent(case.input)\n",
    "    telemetry.tracer_provider.force_flush()\n",
    "    finished_spans = telemetry.in_memory_exporter.get_finished_spans()\n",
    "    mapper = StrandsInMemorySessionMapper()\n",
    "    session = mapper.map_to_session(finished_spans, session_id=session_id)\n",
    "    return {\"output\": str(response), \"trajectory\": session}\n",
    "\n",
    "def billing_support_task(case: Case) -> dict:\n",
    "    \"\"\"Call billing support agent directly with telemetry capture.\"\"\"\n",
    "    telemetry.in_memory_exporter.clear()\n",
    "    session_id = str(uuid.uuid4())\n",
    "    agent = Agent(\n",
    "        name=\"billing_support\",\n",
    "        model=DEFAULT_MODEL,\n",
    "        system_prompt=\"You are a billing specialist. Help with payment issues, subscription questions, refunds, and account billing problems. Use tools to look up customer information.\",\n",
    "        tools=[lookup_customer],\n",
    "        trace_attributes={\"session.id\": session_id},\n",
    "        callback_handler=None\n",
    "    )\n",
    "    response = agent(case.input)\n",
    "    telemetry.tracer_provider.force_flush()\n",
    "    finished_spans = telemetry.in_memory_exporter.get_finished_spans()\n",
    "    mapper = StrandsInMemorySessionMapper()\n",
    "    session = mapper.map_to_session(finished_spans, session_id=session_id)\n",
    "    return {\"output\": str(response), \"trajectory\": session}\n",
    "\n",
    "def product_info_task(case: Case) -> dict:\n",
    "    \"\"\"Call product info agent directly with telemetry capture.\"\"\"\n",
    "    telemetry.in_memory_exporter.clear()\n",
    "    session_id = str(uuid.uuid4())\n",
    "    agent = Agent(\n",
    "        name=\"product_info\",\n",
    "        model=DEFAULT_MODEL,\n",
    "        system_prompt=\"You are a product specialist. Explain product features, capabilities, and help users understand how to use products effectively. Use tools to look up product details.\",\n",
    "        tools=[lookup_product],\n",
    "        trace_attributes={\"session.id\": session_id},\n",
    "        callback_handler=None\n",
    "    )\n",
    "    response = agent(case.input)\n",
    "    telemetry.tracer_provider.force_flush()\n",
    "    finished_spans = telemetry.in_memory_exporter.get_finished_spans()\n",
    "    mapper = StrandsInMemorySessionMapper()\n",
    "    session = mapper.map_to_session(finished_spans, session_id=session_id)\n",
    "    return {\"output\": str(response), \"trajectory\": session}\n",
    "\n",
    "def returns_exchanges_task(case: Case) -> dict:\n",
    "    \"\"\"Call returns agent directly with telemetry capture.\"\"\"\n",
    "    telemetry.in_memory_exporter.clear()\n",
    "    session_id = str(uuid.uuid4())\n",
    "    agent = Agent(\n",
    "        name=\"returns_exchanges\",\n",
    "        model=DEFAULT_MODEL,\n",
    "        system_prompt=\"You are a returns specialist. Help with product returns, exchanges, order modifications, and shipping issues. Use tools to look up order and customer information.\",\n",
    "        tools=[lookup_order, lookup_customer],\n",
    "        trace_attributes={\"session.id\": session_id},\n",
    "        callback_handler=None\n",
    "    )\n",
    "    response = agent(case.input)\n",
    "    telemetry.tracer_provider.force_flush()\n",
    "    finished_spans = telemetry.in_memory_exporter.get_finished_spans()\n",
    "    mapper = StrandsInMemorySessionMapper()\n",
    "    session = mapper.map_to_session(finished_spans, session_id=session_id)\n",
    "    return {\"output\": str(response), \"trajectory\": session}\n",
    "\n",
    "# Create 8 datasets (4 agents x 2 evaluators each)\n",
    "tech_output_dataset = Dataset(\n",
    "    cases=[individual_test_cases[0]],\n",
    "    evaluator=output_evaluator\n",
    ")\n",
    "\n",
    "tech_tool_dataset = Dataset(\n",
    "    cases=[individual_test_cases[0]],\n",
    "    evaluator=tool_selection_evaluator\n",
    ")\n",
    "\n",
    "billing_output_dataset = Dataset(\n",
    "    cases=[individual_test_cases[1]],\n",
    "    evaluator=output_evaluator\n",
    ")\n",
    "\n",
    "billing_tool_dataset = Dataset(\n",
    "    cases=[individual_test_cases[1]],\n",
    "    evaluator=tool_selection_evaluator\n",
    ")\n",
    "\n",
    "product_output_dataset = Dataset(\n",
    "    cases=[individual_test_cases[2]],\n",
    "    evaluator=output_evaluator\n",
    ")\n",
    "\n",
    "product_tool_dataset = Dataset(\n",
    "    cases=[individual_test_cases[2]],\n",
    "    evaluator=tool_selection_evaluator\n",
    ")\n",
    "\n",
    "returns_output_dataset = Dataset(\n",
    "    cases=[individual_test_cases[3]],\n",
    "    evaluator=output_evaluator\n",
    ")\n",
    "\n",
    "returns_tool_dataset = Dataset(\n",
    "    cases=[individual_test_cases[3]],\n",
    "    evaluator=tool_selection_evaluator\n",
    ")\n",
    "\n",
    "print(\"Evaluating individual agent performance...\\n\")\n",
    "\n",
    "# Run 8 separate evaluations\n",
    "tech_output_report = tech_output_dataset.run_evaluations(technical_support_task)\n",
    "tech_tool_report = tech_tool_dataset.run_evaluations(technical_support_task)\n",
    "billing_output_report = billing_output_dataset.run_evaluations(billing_support_task)\n",
    "billing_tool_report = billing_tool_dataset.run_evaluations(billing_support_task)\n",
    "product_output_report = product_output_dataset.run_evaluations(product_info_task)\n",
    "product_tool_report = product_tool_dataset.run_evaluations(product_info_task)\n",
    "returns_output_report = returns_output_dataset.run_evaluations(returns_exchanges_task)\n",
    "returns_tool_report = returns_tool_dataset.run_evaluations(returns_exchanges_task)\n",
    "\n",
    "print(\"Individual agent evaluations complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Individual Performance Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DIMENSION 1: INDIVIDUAL AGENT PERFORMANCE\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "print(\"Technical Support Agent:\")\n",
    "print(\"-\" * 40)\n",
    "print(\"\\nOutput Quality:\")\n",
    "tech_output_report.run_display()\n",
    "print(\"\\nTool Selection:\")\n",
    "tech_tool_report.run_display()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "print(\"Billing Support Agent:\")\n",
    "print(\"-\" * 40)\n",
    "print(\"\\nOutput Quality:\")\n",
    "billing_output_report.run_display()\n",
    "print(\"\\nTool Selection:\")\n",
    "billing_tool_report.run_display()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "print(\"Product Info Agent:\")\n",
    "print(\"-\" * 40)\n",
    "print(\"\\nOutput Quality:\")\n",
    "product_output_report.run_display()\n",
    "print(\"\\nTool Selection:\")\n",
    "product_tool_report.run_display()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "print(\"Returns & Exchanges Agent:\")\n",
    "print(\"-\" * 40)\n",
    "print(\"\\nOutput Quality:\")\n",
    "returns_output_report.run_display()\n",
    "print(\"\\nTool Selection:\")\n",
    "returns_tool_report.run_display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimension 2: Collective System Performance\n",
    "\n",
    "Next, we'll evaluate the complete multi-agent system's performance when the orchestrator routes queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Test Cases for System-Level Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cases for collective system evaluation\n",
    "system_test_cases = [\n",
    "    Case(\n",
    "        name=\"Technical Query - App Crash\",\n",
    "        input=\"My app keeps crashing when I try to login for user123\",\n",
    "        expected_output=\"System should route to technical support and provide troubleshooting help\"\n",
    "    ),\n",
    "    Case(\n",
    "        name=\"Billing Query - Double Charge\",\n",
    "        input=\"I was charged twice this month, my customer ID is user456\",\n",
    "        expected_output=\"System should route to billing support and investigate the charge issue\"\n",
    "    ),\n",
    "    Case(\n",
    "        name=\"Product Query - Features\",\n",
    "        input=\"What features are included in the pro_plan?\",\n",
    "        expected_output=\"System should route to product info and explain pro_plan features\"\n",
    "    ),\n",
    "    Case(\n",
    "        name=\"Order Query - Status Check\",\n",
    "        input=\"I want to check the status of order789\",\n",
    "        expected_output=\"System should route to returns specialist and provide order status\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "print(f\"Created {len(system_test_cases)} test cases for system-level evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create System-Level Evaluators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output evaluator for final system response quality\n",
    "system_output_evaluator = OutputEvaluator(\n",
    "    rubric=\"Evaluate the quality and completeness of the multi-agent system's final response.\",\n",
    "    model=DEFAULT_MODEL\n",
    ")\n",
    "\n",
    "print(\"System-level evaluator created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Complete System Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define system task (using orchestrator)\n",
    "def system_task(case: Case) -> str:\n",
    "    \"\"\"Execute query through the complete multi-agent system.\"\"\"\n",
    "    # Reset orchestrator messages for clean execution\n",
    "    orchestrator.messages = []\n",
    "    response = orchestrator(case.input)\n",
    "    return str(response)\n",
    "\n",
    "# Create dataset for system evaluation\n",
    "system_dataset = Dataset(\n",
    "    cases=system_test_cases,\n",
    "    evaluator=system_output_evaluator\n",
    ")\n",
    "\n",
    "print(\"Evaluating complete system performance...\\n\")\n",
    "\n",
    "# Run evaluation\n",
    "system_report = system_dataset.run_evaluations(system_task)\n",
    "\n",
    "print(\"System evaluation complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display System Performance Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DIMENSION 2: COLLECTIVE SYSTEM PERFORMANCE\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "system_report.run_display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimension 3: Coordination Quality\n",
    "\n",
    "Finally, we'll evaluate how well the orchestrator coordinates with specialists - analyzing routing decisions and handoff quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding InteractionsEvaluator\n",
    "\n",
    "**InteractionsEvaluator** is specifically designed to evaluate multi-agent interactions:\n",
    "\n",
    "- **Node Name**: Which agent handled the interaction\n",
    "- **Dependencies**: What information was passed to the agent\n",
    "- **Messages**: The output or result from the agent\n",
    "\n",
    "This evaluator assesses:\n",
    "- Whether the right agent was selected\n",
    "- If information was passed correctly\n",
    "- Whether the agent's response was appropriate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Coordination Evaluation Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define coordination task that captures interactions\n",
    "def coordination_task(case: Case) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Execute query and capture interaction data for coordination evaluation.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with output, trajectory, and interactions\n",
    "    \"\"\"\n",
    "    # Reset orchestrator messages\n",
    "    orchestrator.messages = []\n",
    "    \n",
    "    # Execute query\n",
    "    response = orchestrator(case.input)\n",
    "    \n",
    "    # Extract tools used from messages\n",
    "    tools_used = tools_use_extractor.extract_agent_tools_used_from_messages(orchestrator.messages)\n",
    "    \n",
    "    # Build interactions list\n",
    "    interactions = []\n",
    "    for tool_used in tools_used:\n",
    "        interactions.append(\n",
    "            Interaction(\n",
    "                node_name=tool_used[\"name\"],\n",
    "                dependencies=[tool_used[\"input\"]],\n",
    "                messages=[tool_used[\"tool_result\"]]\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    return {\n",
    "        \"output\": str(response),\n",
    "        \"trajectory\": tools_used,\n",
    "        \"interactions\": interactions\n",
    "    }\n",
    "\n",
    "print(\"Coordination task function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create InteractionsEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interaction evaluator with custom rubric\n",
    "coordination_rubric = \"\"\"\n",
    "Evaluate the orchestrator's routing decision and the specialist agent's response quality:\n",
    "\n",
    "Score 1.0 if:\n",
    "- The orchestrator routed to the correct specialist for the query type\n",
    "- The specialist used appropriate tools for their domain\n",
    "- The specialist provided a complete, helpful response\n",
    "\n",
    "Score 0.7 if:\n",
    "- Correct specialist selected\n",
    "- Most appropriate tools used\n",
    "- Response is adequate but could be more complete\n",
    "\n",
    "Score 0.4 if:\n",
    "- Marginally correct specialist or could have chosen better\n",
    "- Some tool usage issues\n",
    "- Response partially addresses the query\n",
    "\n",
    "Score 0.0 if:\n",
    "- Wrong specialist selected for the query type\n",
    "- Inappropriate tool usage\n",
    "- Response does not address the query\n",
    "\"\"\"\n",
    "\n",
    "interaction_evaluator = InteractionsEvaluator(\n",
    "    rubric=coordination_rubric,\n",
    "    model=DEFAULT_MODEL\n",
    ")\n",
    "\n",
    "# Get tool descriptions for context\n",
    "test_execution = coordination_task(system_test_cases[0])\n",
    "tool_description = tools_use_extractor.extract_tools_description(orchestrator)\n",
    "interaction_evaluator.update_interaction_description(tool_description)\n",
    "\n",
    "print(\"InteractionsEvaluator created with custom rubric\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Coordination Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset for coordination evaluation\n",
    "coordination_dataset = Dataset(\n",
    "    cases=system_test_cases,\n",
    "    evaluator=interaction_evaluator\n",
    ")\n",
    "\n",
    "print(\"Evaluating agent coordination quality...\\n\")\n",
    "\n",
    "# Run evaluation\n",
    "coordination_report = coordination_dataset.run_evaluations(coordination_task)\n",
    "\n",
    "print(\"Coordination evaluation complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Coordination Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DIMENSION 3: COORDINATION QUALITY\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "coordination_report.run_display(include_actual_interactions=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Agent Evaluation Report\n",
    "\n",
    "Let's compile all three evaluation dimensions into a comprehensive multi-agent evaluation report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile comprehensive evaluation report\n",
    "def calculate_average_score(report):\n",
    "    \"\"\"Calculate average score from evaluation report.\"\"\"\n",
    "    if report.scores:\n",
    "        return sum(report.scores) / len(report.scores)\n",
    "    return 0.0\n",
    "\n",
    "# Calculate metrics for each dimension\n",
    "tech_output_score = calculate_average_score(tech_output_report)\n",
    "tech_tool_score = calculate_average_score(tech_tool_report)\n",
    "billing_output_score = calculate_average_score(billing_output_report)\n",
    "billing_tool_score = calculate_average_score(billing_tool_report)\n",
    "product_output_score = calculate_average_score(product_output_report)\n",
    "product_tool_score = calculate_average_score(product_tool_report)\n",
    "returns_output_score = calculate_average_score(returns_output_report)\n",
    "returns_tool_score = calculate_average_score(returns_tool_report)\n",
    "\n",
    "tech_score = (tech_output_score + tech_tool_score) / 2\n",
    "billing_score = (billing_output_score + billing_tool_score) / 2\n",
    "product_score = (product_output_score + product_tool_score) / 2\n",
    "returns_score = (returns_output_score + returns_tool_score) / 2\n",
    "individual_avg = (tech_score + billing_score + product_score + returns_score) / 4\n",
    "\n",
    "system_score = calculate_average_score(system_report)\n",
    "coordination_score = calculate_average_score(coordination_report)\n",
    "\n",
    "# Create comprehensive report\n",
    "comprehensive_report = f\"\"\"\n",
    "# Multi-Agent System Evaluation Report\n",
    "\n",
    "## Executive Summary\n",
    "\n",
    "This report presents a comprehensive evaluation of the customer support multi-agent system across three critical dimensions: individual agent performance, collective system performance, and coordination quality.\n",
    "\n",
    "## Overall Metrics\n",
    "\n",
    "| Dimension | Score | Status |\n",
    "|:----------|:------|:-------|\n",
    "| **Individual Performance** | {individual_avg:.2f} | {'\u2705 Good' if individual_avg >= 0.7 else '\u274c Needs Improvement'} |\n",
    "| **Collective Performance** | {system_score:.2f} | {'\u2705 Good' if system_score >= 0.7 else '\u274c Needs Improvement'} |\n",
    "| **Coordination Quality** | {coordination_score:.2f} | {'\u2705 Good' if coordination_score >= 0.7 else '\u274c Needs Improvement'} |\n",
    "| **Overall System Score** | {(individual_avg + system_score + coordination_score) / 3:.2f} | {'\u2705 Production Ready' if (individual_avg + system_score + coordination_score) / 3 >= 0.7 else '\u274c Requires Optimization'} |\n",
    "\n",
    "## Dimension 1: Individual Agent Performance\n",
    "\n",
    "### Specialist Agent Scores\n",
    "\n",
    "| Agent | Score | Assessment |\n",
    "|:------|:------|:-----------|\n",
    "| Technical Support | {tech_score:.2f} | {'Strong performance' if tech_score >= 0.7 else 'Needs improvement'} |\n",
    "| Billing Support | {billing_score:.2f} | {'Strong performance' if billing_score >= 0.7 else 'Needs improvement'} |\n",
    "| Product Info | {product_score:.2f} | {'Strong performance' if product_score >= 0.7 else 'Needs improvement'} |\n",
    "| Returns & Exchanges | {returns_score:.2f} | {'Strong performance' if returns_score >= 0.7 else 'Needs improvement'} |\n",
    "\n",
    "**Key Findings:**\n",
    "- Each specialist agent demonstrates competence in their domain\n",
    "- Tool selection accuracy varies by agent and query complexity\n",
    "- Response quality is generally consistent across specialists\n",
    "\n",
    "## Dimension 2: Collective System Performance\n",
    "\n",
    "**System Score:** {system_score:.2f}\n",
    "\n",
    "**Key Findings:**\n",
    "- The complete system produces appropriate responses when routing correctly\n",
    "- End-to-end query handling meets quality standards\n",
    "- User-facing output is coherent and helpful\n",
    "\n",
    "## Dimension 3: Coordination Quality\n",
    "\n",
    "**Coordination Score:** {coordination_score:.2f}\n",
    "\n",
    "**Key Findings:**\n",
    "- Orchestrator routing decisions are generally accurate\n",
    "- Handoffs between orchestrator and specialists are smooth\n",
    "- Information is passed correctly between agents\n",
    "- Interaction quality supports effective collaboration\n",
    "\n",
    "## Coordination Metrics\n",
    "\n",
    "### Routing Accuracy Analysis\n",
    "\n",
    "**Expected Routing:**\n",
    "- Technical issues -> technical_support\n",
    "- Billing questions -> billing_support\n",
    "- Product queries -> product_info\n",
    "- Order issues -> returns_exchanges\n",
    "\n",
    "### Handoff Quality Indicators\n",
    "\n",
    "- **Context Preservation**: Query context maintained during handoffs\n",
    "- **Information Completeness**: Specialists receive all necessary information\n",
    "- **Response Integration**: Final responses integrate specialist outputs effectively\n",
    "\n",
    "## Recommendations\n",
    "\n",
    "### Strengths\n",
    "1. Clear separation of concerns among specialists\n",
    "2. Effective orchestrator routing logic\n",
    "3. Appropriate tool usage by domain experts\n",
    "\n",
    "### Areas for Improvement\n",
    "1. **Individual Agents**: {'Optimize tool selection for edge cases' if individual_avg < 0.85 else 'Maintain current performance levels'}\n",
    "2. **System Level**: {'Improve orchestrator prompts for better routing' if system_score < 0.85 else 'Consider expanding capabilities'}\n",
    "3. **Coordination**: {'Enhance handoff protocols between agents' if coordination_score < 0.85 else 'Document successful patterns'}\n",
    "\n",
    "### Next Steps\n",
    "1. Monitor production performance with real user queries\n",
    "2. Expand test coverage to include edge cases and complex scenarios\n",
    "3. Implement continuous evaluation pipeline\n",
    "4. Gather user feedback to validate evaluation metrics\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "The multi-agent customer support system demonstrates {'strong' if (individual_avg + system_score + coordination_score) / 3 >= 0.7 else 'moderate'} performance across all evaluation dimensions. The combination of specialized agents with effective orchestration creates a {'production-ready' if (individual_avg + system_score + coordination_score) / 3 >= 0.7 else 'promising'} system for handling diverse customer support queries.\n",
    "\n",
    "**Overall Assessment:** {'READY FOR DEPLOYMENT' if (individual_avg + system_score + coordination_score) / 3 >= 0.75 else 'REQUIRES ADDITIONAL OPTIMIZATION'}\n",
    "\"\"\"\n",
    "\n",
    "display(Markdown(comprehensive_report))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices for Multi-Agent Evaluation\n",
    "\n",
    "### Always Evaluate All Three Dimensions\n",
    "\n",
    "| Dimension | Purpose | Key Tests |\n",
    "|:----------|:--------|:----------|\n",
    "| Individual Performance | Ensure specialist competence | Isolation tests, tool selection, response quality |\n",
    "| Collective Performance | Validate end-to-end quality | Complete workflows, final output, task completion |\n",
    "| Coordination Quality | Evaluate collaboration | Routing decisions, handoff quality, information flow |\n",
    "\n",
    "### When to Use InteractionsEvaluator\n",
    "\n",
    "Use when: evaluating multi-agent systems, analyzing collaboration patterns, debugging coordination issues, validating information flow between agents.\n",
    "\n",
    "### Interpreting Results\n",
    "\n",
    "| Pattern | Diagnosis |\n",
    "|:--------|:----------|\n",
    "| High individual, low collective | Orchestration issues |\n",
    "| High collective, low coordination | Correct outcomes despite poor process |\n",
    "| Low individual, any collective | Specialist agents need improvement |\n",
    "| High all dimensions | System well-designed and functioning |\n",
    "\n",
    "### Production Monitoring\n",
    "\n",
    "Track: routing accuracy rate, average handoffs per query, specialist utilization, end-to-end latency, user satisfaction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You've successfully learned how to evaluate multi-agent systems using Strands Evals. You now understand:\n",
    "\n",
    "- **Multi-agent architectures**: Agent-as-tool pattern with orchestrator and specialists\n",
    "- **Three evaluation dimensions**:\n",
    "  - Individual Performance: Each agent evaluated separately\n",
    "  - Collective Performance: Complete system evaluated as a whole\n",
    "  - Coordination Quality: Agent collaboration and handoffs\n",
    "- **InteractionsEvaluator**: Specialized evaluator for multi-agent coordination\n",
    "- **Interaction structure**: Node names, dependencies, and messages\n",
    "- **Comprehensive reporting**: Combining multiple evaluation dimensions\n",
    "- **Coordination metrics**: Routing accuracy, handoff quality, information flow\n",
    "- **Best practices**: When and how to evaluate multi-agent systems effectively\n",
    "\n",
    "Multi-agent evaluation is essential for building reliable, efficient, and collaborative AI systems. By evaluating individual components, collective outcomes, and coordination quality, you gain complete visibility into complex multi-agent architectures."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai-on-aws",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}