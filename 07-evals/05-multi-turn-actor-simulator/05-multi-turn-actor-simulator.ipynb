{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "## Multi-turn Evaluation using Actor Simulator - Realistic Conversational Agent Testing\n",
    "\n",
    "This tutorial demonstrates how to use ActorSimulator to evaluate conversational agents through realistic multi-turn interactions. ActorSimulator creates AI-powered user personas that engage with your agent naturally, helping you test complex conversation flows, handle diverse user behaviors, and ensure robust agent performance across different scenarios.\n",
    "\n",
    "### What You'll Learn\n",
    "- Create realistic user simulations with ActorSimulator API\n",
    "- Design actor profiles with traits, context, and goals\n",
    "- Implement goal completion detection with stop tokens\n",
    "- Test agents with diverse personas (polite, demanding, confused)\n",
    "- Build automated multi-turn evaluation pipelines\n",
    "- Implement dev-to-prod workflow with metric comparison\n",
    "- Scale testing with datasets and automated simulation\n",
    "\n",
    "### Tutorial Details\n",
    "\n",
    "| Information         | Details                                                                       |\n",
    "|:--------------------|:------------------------------------------------------------------------------|\n",
    "| Tutorial type       | Advanced - Realistic multi-turn conversation testing                         |\n",
    "| Tutorial components | ActorSimulator, Personal Assistant agent, multi-turn conversations           |\n",
    "| Tutorial vertical   | Agent Evaluation                                                              |\n",
    "| Example complexity  | Advanced                                                                      |\n",
    "| SDK used            | Strands Agents, Strands Evals                                                 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": "### Understanding Multi-turn Evaluation\n\nMulti-turn evaluation tests conversational agents through realistic interactions, verifying context maintenance and handling of diverse user behaviors.\n\n#### Why Multi-turn Evaluation Matters\n\n| Single-turn Limitations | Multi-turn Benefits |\n|:------------------------|:--------------------|\n| Can't test context maintenance | Verifies conversation memory |\n| Misses follow-up handling | Tests clarification flows |\n| Same user style only | Evaluates diverse personas |\n| No edge case discovery | Finds conversation-specific issues |\n\n#### ActorSimulator Architecture\n\nActorSimulator creates AI-powered user personas that:\n- Understand goals and know what they're trying to achieve\n- Communicate with specific personality traits\n- Maintain context and remember conversation history\n- Signal completion using `<stop/>` token when satisfied\n\n#### Actor Profile Components\n\n| Component | Purpose |\n|:----------|:--------|\n| Traits | Personality characteristics (polite, demanding, confused) |\n| Context | Background information and current situation |\n| Goals | What the actor wants to accomplish |\n| Task description | Clear success criteria for goal completion |"
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "### Environment Setup\n",
    "\n",
    "Configure AWS region and model settings for this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# AWS Configuration (inline - no config.py)\n",
    "session = boto3.Session()\n",
    "AWS_REGION = session.region_name or 'us-east-1'\n",
    "DEFAULT_MODEL = 'us.anthropic.claude-3-7-sonnet-20250219-v1:0'\n",
    "\n",
    "print(f\"AWS Region: {AWS_REGION}\")\n",
    "print(f\"Model: {DEFAULT_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "### Setup and Imports\n",
    "\n",
    "Import all necessary libraries for agent creation, actor simulation, and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import json\n",
    "import sqlite3\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# Strands imports\n",
    "from strands import Agent, tool\n",
    "from strands.models import BedrockModel\n",
    "\n",
    "# Strands Evals imports\n",
    "from strands_evals import Dataset, Case\n",
    "from strands_evals.simulation import ActorSimulator\n",
    "\n",
    "# Display utilities\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": "### Personal Assistant Agent System\n\nWe'll use a simplified Personal Assistant agent for multi-turn evaluation. This agent helps users manage their calendar with appointment scheduling and querying capabilities.\n\n#### Agent Architecture\n\n```\nPersonal Assistant\n  \u2514\u2500\u2500 Calendar Tools\n        \u251c\u2500\u2500 create_appointment\n        \u251c\u2500\u2500 list_appointments\n        \u2514\u2500\u2500 get_today_date\n```\n\n#### Agent Code\n\nAgent code adapted from: /strands-samples/02-samples/05-personal-assistant/"
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "### Calendar Tools Implementation\n",
    "\n",
    "Define the calendar management tools that the Personal Assistant will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SQLite database for appointments\n",
    "def init_calendar_db():\n",
    "    \"\"\"Initialize the appointments database.\"\"\"\n",
    "    conn = sqlite3.connect(\"appointments.db\")\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS appointments (\n",
    "            id TEXT PRIMARY KEY,\n",
    "            date TEXT,\n",
    "            location TEXT,\n",
    "            title TEXT,\n",
    "            description TEXT\n",
    "        )\n",
    "    \"\"\")\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "init_calendar_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def create_appointment(date: str, location: str, title: str, description: str) -> str:\n",
    "    \"\"\"\n",
    "    Create a new personal appointment in the database.\n",
    "\n",
    "    Args:\n",
    "        date (str): Date and time of the appointment (format: YYYY-MM-DD HH:MM).\n",
    "        location (str): Location of the appointment.\n",
    "        title (str): Title of the appointment.\n",
    "        description (str): Description of the appointment.\n",
    "\n",
    "    Returns:\n",
    "        str: Formatted confirmation of the newly created appointment.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        datetime.strptime(date, \"%Y-%m-%d %H:%M\")\n",
    "    except ValueError:\n",
    "        return \"Error: Date must be in format 'YYYY-MM-DD HH:MM'\"\n",
    "\n",
    "    appointment_id = str(uuid.uuid4())\n",
    "\n",
    "    conn = sqlite3.connect(\"appointments.db\")\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    cursor.execute(\n",
    "        \"INSERT INTO appointments (id, date, location, title, description) VALUES (?, ?, ?, ?, ?)\",\n",
    "        (appointment_id, date, location, title, description),\n",
    "    )\n",
    "\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "    time_part = date.split(\" \")[1] if \" \" in date else \"No time specified\"\n",
    "    date_part = date.split(\" \")[0] if \" \" in date else date\n",
    "    confirmation = [\n",
    "        \"Appointment Created Successfully!\",\n",
    "        f\"Date: {date_part}\",\n",
    "        f\"Time: {time_part}\",\n",
    "        f\"Location: {location}\",\n",
    "        f\"Title: {title}\",\n",
    "        f\"Description: {description}\",\n",
    "        f\"ID: {appointment_id}\"\n",
    "    ]\n",
    "    return \"\\n\".join(confirmation)\n",
    "\n",
    "\n",
    "@tool\n",
    "def list_appointments() -> str:\n",
    "    \"\"\"\n",
    "    List all appointments in the calendar.\n",
    "\n",
    "    Returns:\n",
    "        str: Formatted list of all appointments.\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(\"appointments.db\")\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    cursor.execute(\"SELECT id, date, location, title, description FROM appointments ORDER BY date\")\n",
    "    appointments = cursor.fetchall()\n",
    "    conn.close()\n",
    "\n",
    "    if not appointments:\n",
    "        return \"No appointments found.\"\n",
    "\n",
    "    result = [\"Your Appointments:\"]\n",
    "    for apt in appointments:\n",
    "        apt_id, date, location, title, description = apt\n",
    "        result.append(f\"\\n[{apt_id}]\")\n",
    "        result.append(f\"Title: {title}\")\n",
    "        result.append(f\"Date: {date}\")\n",
    "        result.append(f\"Location: {location}\")\n",
    "        result.append(f\"Description: {description}\")\n",
    "    \n",
    "    return \"\\n\".join(result)\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_today_date() -> str:\n",
    "    \"\"\"\n",
    "    Get today's date in YYYY-MM-DD format.\n",
    "\n",
    "    Returns:\n",
    "        str: Today's date.\n",
    "    \"\"\"\n",
    "    return datetime.now().strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "### Create Personal Assistant Agent\n",
    "\n",
    "Build the Personal Assistant agent with calendar management capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Personal Assistant agent\n",
    "model = BedrockModel(\n",
    "    model_id=DEFAULT_MODEL,\n",
    ")\n",
    "\n",
    "personal_assistant = Agent(\n",
    "    name=\"PersonalAssistant\",\n",
    "    model=model,\n",
    "    system_prompt=\"\"\"You are a helpful personal assistant specializing in calendar management.\n",
    "    \n",
    "You can help users:\n",
    "- Create new appointments\n",
    "- List existing appointments\n",
    "- Check today's date\n",
    "\n",
    "Always be helpful, clear, and concise. When creating appointments, confirm all details back to the user.\n",
    "For dates, use the format YYYY-MM-DD HH:MM.\"\"\",\n",
    "    tools=[create_appointment, list_appointments, get_today_date]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "### Test the Personal Assistant\n",
    "\n",
    "Before running multi-turn evaluation, let's verify the agent works correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the agent with a simple request\n",
    "test_input = \"What's today's date?\"\n",
    "personal_assistant(test_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "### Scenario 1: Basic User Simulation (Single Persona)\n",
    "\n",
    "Start with a simple scenario: a single polite user trying to schedule an appointment. This demonstrates the basic ActorSimulator API and conversation flow.\n",
    "\n",
    "#### Scenario Details\n",
    "- **User type**: Polite professional\n",
    "- **Goal**: Schedule a dentist appointment\n",
    "- **Expected behavior**: Clear communication, provides all details, confirms completion\n",
    "- **Success criteria**: Appointment created with correct details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test case for basic user simulation\n",
    "basic_case = Case(\n",
    "    name=\"Basic Appointment Scheduling\",\n",
    "    input=\"Hi, I need to schedule a dentist appointment for next Monday at 2pm at Downtown Dental Clinic.\",\n",
    "    metadata={\n",
    "        \"task_description\": \"User successfully schedules a dentist appointment with all required details\",\n",
    "        \"actor_traits\": [\"polite\", \"clear\", \"professional\"],\n",
    "        \"expected_turns\": 3\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ActorSimulator from test case\n",
    "basic_actor = ActorSimulator.from_case_for_user_simulator(\n",
    "    case=basic_case,\n",
    "    max_turns=5\n",
    ")\n",
    "\n",
    "print(f\"Goal: {basic_case.metadata['task_description']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the multi-turn conversation\n",
    "conversation_history = []\n",
    "user_message = basic_case.input\n",
    "turn = 0\n",
    "\n",
    "while basic_actor.has_next():\n",
    "    turn += 1\n",
    "    print(f\"\\nTURN {turn}\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    # Display user message\n",
    "    print(f\"\\nUSER: {user_message}\")\n",
    "    conversation_history.append({\"role\": \"user\", \"content\": user_message})\n",
    "    \n",
    "    # Agent responds\n",
    "    agent_response = personal_assistant(user_message)\n",
    "    print(f\"\\nAGENT: {agent_response}\")\n",
    "    conversation_history.append({\"role\": \"agent\", \"content\": str(agent_response)})\n",
    "    \n",
    "    # Actor responds\n",
    "    actor_result = basic_actor.act(str(agent_response))\n",
    "    user_message = str(actor_result.structured_output.message)\n",
    "    \n",
    "    # Check for completion\n",
    "    if \"<stop/>\" in user_message:\n",
    "        print(f\"\\nUSER: {user_message.replace('<stop/>', '[GOAL COMPLETED]')}\")\n",
    "        conversation_history.append({\"role\": \"user\", \"content\": user_message})\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "### Conversation Analysis\n",
    "\n",
    "Analyze the conversation to understand actor behavior and goal completion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the conversation\n",
    "analysis = f\"\"\"\n",
    "## Basic User Simulation Analysis\n",
    "\n",
    "**Conversation Metrics:**\n",
    "- Total turns: {turn}\n",
    "- Total messages: {len(conversation_history)}\n",
    "- Expected turns: {basic_case.metadata['expected_turns']}\n",
    "- Efficiency: {'On target' if turn <= basic_case.metadata['expected_turns'] else 'Exceeded expected'}\n",
    "\n",
    "**Actor Behavior:**\n",
    "- Persona: {', '.join(basic_case.metadata['actor_traits'])}\n",
    "- Goal completion: {'Success' if '<stop/>' in user_message else 'Incomplete'}\n",
    "- Communication style: Clear and professional\n",
    "\n",
    "**Key Observations:**\n",
    "1. Actor maintained consistent personality throughout conversation\n",
    "2. Goal completion token emitted when appointment was confirmed\n",
    "3. Natural conversation flow with appropriate follow-ups\n",
    "4. Agent successfully handled the request and confirmed details\n",
    "\n",
    "**Success Criteria:**\n",
    "- Appointment created: {'Yes' if any('Appointment Created' in msg['content'] for msg in conversation_history) else 'No'}\n",
    "- Details confirmed: {'Yes' if any('dentist' in msg['content'].lower() for msg in conversation_history) else 'No'}\n",
    "- User satisfied: {'Yes' if '<stop/>' in user_message else 'No'}\n",
    "\"\"\"\n",
    "\n",
    "display(Markdown(analysis))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "### Scenario 2: Diverse Personas\n",
    "\n",
    "Test the agent with three different user personas to evaluate robustness:\n",
    "1. **Polite User**: Courteous and clear communication\n",
    "2. **Demanding User**: Impatient and expects immediate results\n",
    "3. **Confused User**: Unclear requests and needs clarification\n",
    "\n",
    "This scenario demonstrates how ActorSimulator can generate varied user behaviors to test agent resilience. Use this approach when you need to validate that your agent can handle different communication styles and personalities that you'll encounter in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test cases for diverse personas\n",
    "persona_cases = [\n",
    "    Case(\n",
    "        name=\"Polite User\",\n",
    "        input=\"Good morning! I'd like to schedule a team meeting for Thursday at 10am in Conference Room A. It's a project planning session.\",\n",
    "        metadata={\n",
    "            \"task_description\": \"User politely schedules a team meeting and confirms details\",\n",
    "            \"persona\": \"polite\",\n",
    "            \"traits\": [\"courteous\", \"professional\", \"patient\"]\n",
    "        }\n",
    "    ),\n",
    "    Case(\n",
    "        name=\"Demanding User\",\n",
    "        input=\"I need a meeting scheduled NOW. Tomorrow, 3pm, Board Room. Client presentation. Make it happen.\",\n",
    "        metadata={\n",
    "            \"task_description\": \"User demands immediate scheduling and expects quick confirmation\",\n",
    "            \"persona\": \"demanding\",\n",
    "            \"traits\": [\"impatient\", \"direct\", \"urgent\"]\n",
    "        }\n",
    "    ),\n",
    "    Case(\n",
    "        name=\"Confused User\",\n",
    "        input=\"Um, I think I need to set up something... maybe a meeting? Or was it a call? I'm not sure about the time...\",\n",
    "        metadata={\n",
    "            \"task_description\": \"User is confused but eventually provides enough details to schedule an appointment\",\n",
    "            \"persona\": \"confused\",\n",
    "            \"traits\": [\"uncertain\", \"vague\", \"needs-guidance\"]\n",
    "        }\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run multi-turn conversations for all personas\n",
    "persona_results = []\n",
    "\n",
    "for case in persona_cases:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"PERSONA: {case.name.upper()}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Create actor for this persona\n",
    "    actor = ActorSimulator.from_case_for_user_simulator(\n",
    "        case=case,\n",
    "        max_turns=5\n",
    "    )\n",
    "    \n",
    "    conversation = []\n",
    "    user_message = case.input\n",
    "    turn = 0\n",
    "    \n",
    "    while actor.has_next() and turn < 5:\n",
    "        turn += 1\n",
    "        print(f\"\\n[Turn {turn}]\")\n",
    "        \n",
    "        # User message\n",
    "        print(f\"USER: {user_message}\")\n",
    "        conversation.append({\"role\": \"user\", \"content\": user_message})\n",
    "        \n",
    "        # Agent response\n",
    "        agent_response = personal_assistant(user_message)\n",
    "        print(f\"AGENT: {agent_response}\")\n",
    "        conversation.append({\"role\": \"agent\", \"content\": str(agent_response)})\n",
    "        \n",
    "        # Actor response\n",
    "        actor_result = actor.act(str(agent_response))\n",
    "        user_message = str(actor_result.structured_output.message)\n",
    "        \n",
    "        # Check for completion\n",
    "        if \"<stop/>\" in user_message:\n",
    "            print(f\"\\n[Goal Completed]\")\n",
    "            conversation.append({\"role\": \"user\", \"content\": user_message})\n",
    "            break\n",
    "    \n",
    "    # Store results\n",
    "    persona_results.append({\n",
    "        \"case\": case,\n",
    "        \"conversation\": conversation,\n",
    "        \"turns\": turn,\n",
    "        \"completed\": \"<stop/>\" in user_message\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "### Persona Comparison Analysis\n",
    "\n",
    "Compare how the agent handled different user personas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\n",
    "comparison_md = \"\"\"\n",
    "## Persona Evaluation Comparison\n",
    "\n",
    "| Persona | Turns | Completed | Avg Message Length | Traits |\n",
    "|:--------|:------|:----------|:-------------------|:-------|\n",
    "\"\"\"\n",
    "\n",
    "for result in persona_results:\n",
    "    case = result['case']\n",
    "    avg_length = sum(len(msg['content']) for msg in result['conversation']) / len(result['conversation'])\n",
    "    traits = ', '.join(case.metadata['traits'])\n",
    "    completed_icon = \"Yes\" if result['completed'] else \"No\"\n",
    "    \n",
    "    comparison_md += f\"| {case.name} | {result['turns']} | {completed_icon} | {avg_length:.0f} chars | {traits} |\\n\"\n",
    "\n",
    "comparison_md += \"\"\"\n",
    "\n",
    "### Key Insights\n",
    "\n",
    "**Polite User:**\n",
    "- Most efficient conversation flow\n",
    "- Clear communication reduces back-and-forth\n",
    "- Agent responds naturally to courteous tone\n",
    "- Fastest goal completion\n",
    "\n",
    "**Demanding User:**\n",
    "- Direct communication style\n",
    "- Agent maintains professionalism despite urgency\n",
    "- Similar efficiency to polite user\n",
    "- Demonstrates agent's ability to handle pressure\n",
    "\n",
    "**Confused User:**\n",
    "- Requires more clarification turns\n",
    "- Agent successfully guides user to provide details\n",
    "- More messages exchanged due to vagueness\n",
    "- Tests agent's patience and clarification abilities\n",
    "\n",
    "### Agent Resilience Assessment\n",
    "\n",
    "**Strengths:**\n",
    "- Handles diverse communication styles effectively\n",
    "- Maintains professionalism across all personas\n",
    "- Successfully guides confused users to goal completion\n",
    "- Adapts tone appropriately to user style\n",
    "\n",
    "**Observations:**\n",
    "- Turn count varies by persona (2-5 turns typical)\n",
    "- All personas achieved goal completion\n",
    "- Agent robust to different user behaviors\n",
    "- No significant failures across persona types\n",
    "\"\"\"\n",
    "\n",
    "display(Markdown(comparison_md))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": "### Scenario 3: Dataset + Simulation Pipeline\n\nScale up evaluation by creating a dataset of test cases and running automated simulations. This demonstrates production-ready evaluation workflows.\n\n#### Pipeline Architecture\n\n```\nDataset (multiple cases)\n  \u2192 For each case: Create ActorSimulator \u2192 Run conversation \u2192 Collect metrics\n  \u2192 Aggregate Results (success rate, avg turns, error analysis)\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create evaluation dataset\n",
    "evaluation_cases = [\n",
    "    Case(\n",
    "        name=\"Schedule Doctor Appointment\",\n",
    "        input=\"I need to book a doctor's appointment for next Friday at 9am at City Medical Center.\",\n",
    "        metadata={\n",
    "            \"task_description\": \"Schedule a medical appointment with specific date, time, and location\",\n",
    "            \"category\": \"healthcare\",\n",
    "            \"complexity\": \"simple\"\n",
    "        }\n",
    "    ),\n",
    "    Case(\n",
    "        name=\"Check Existing Appointments\",\n",
    "        input=\"Can you show me what appointments I have scheduled?\",\n",
    "        metadata={\n",
    "            \"task_description\": \"List all existing appointments in the calendar\",\n",
    "            \"category\": \"query\",\n",
    "            \"complexity\": \"simple\"\n",
    "        }\n",
    "    ),\n",
    "    Case(\n",
    "        name=\"Schedule Multiple Appointments\",\n",
    "        input=\"I need to schedule two appointments: a dentist visit on Monday at 2pm and a haircut on Wednesday at 11am.\",\n",
    "        metadata={\n",
    "            \"task_description\": \"Schedule multiple appointments in a single conversation\",\n",
    "            \"category\": \"multiple-tasks\",\n",
    "            \"complexity\": \"medium\"\n",
    "        }\n",
    "    ),\n",
    "    Case(\n",
    "        name=\"Vague Appointment Request\",\n",
    "        input=\"I think I need to set something up sometime next week...\",\n",
    "        metadata={\n",
    "            \"task_description\": \"Handle vague request and guide user to provide necessary details\",\n",
    "            \"category\": \"clarification\",\n",
    "            \"complexity\": \"medium\"\n",
    "        }\n",
    "    ),\n",
    "    Case(\n",
    "        name=\"Check Date and Schedule\",\n",
    "        input=\"What's today's date? I want to schedule something for three days from now.\",\n",
    "        metadata={\n",
    "            \"task_description\": \"Provide current date and schedule appointment with relative date\",\n",
    "            \"category\": \"multi-step\",\n",
    "            \"complexity\": \"medium\"\n",
    "        }\n",
    "    )\n",
    "]\n",
    "\n",
    "dataset = Dataset(\n",
    "    cases=evaluation_cases\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define multi-turn evaluation function\n",
    "def run_multi_turn_evaluation(case: Case, max_turns: int = 5) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Run a multi-turn conversation evaluation for a single case.\n",
    "    \n",
    "    Args:\n",
    "        case: Test case to evaluate\n",
    "        max_turns: Maximum conversation turns\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing evaluation results\n",
    "    \"\"\"\n",
    "    # Create actor\n",
    "    actor = ActorSimulator.from_case_for_user_simulator(\n",
    "        case=case,\n",
    "        max_turns=max_turns\n",
    "    )\n",
    "    \n",
    "    # Run conversation\n",
    "    conversation = []\n",
    "    user_message = case.input\n",
    "    turn = 0\n",
    "    goal_completed = False\n",
    "    \n",
    "    while actor.has_next() and turn < max_turns:\n",
    "        turn += 1\n",
    "        \n",
    "        # User message\n",
    "        conversation.append({\"role\": \"user\", \"content\": user_message})\n",
    "        \n",
    "        # Agent response\n",
    "        try:\n",
    "            agent_response = personal_assistant(user_message)\n",
    "            conversation.append({\"role\": \"agent\", \"content\": str(agent_response)})\n",
    "        except Exception as e:\n",
    "            conversation.append({\"role\": \"agent\", \"content\": f\"Error: {str(e)}\"})\n",
    "            break\n",
    "        \n",
    "        # Actor response\n",
    "        actor_result = actor.act(str(agent_response))\n",
    "        user_message = str(actor_result.structured_output.message)\n",
    "        \n",
    "        # Check for completion\n",
    "        if \"<stop/>\" in user_message:\n",
    "            goal_completed = True\n",
    "            conversation.append({\"role\": \"user\", \"content\": user_message})\n",
    "            break\n",
    "    \n",
    "    # Calculate metrics\n",
    "    total_messages = len(conversation)\n",
    "    user_messages = [msg for msg in conversation if msg[\"role\"] == \"user\"]\n",
    "    agent_messages = [msg for msg in conversation if msg[\"role\"] == \"agent\"]\n",
    "    \n",
    "    return {\n",
    "        \"case_name\": case.name,\n",
    "        \"category\": case.metadata[\"category\"],\n",
    "        \"complexity\": case.metadata[\"complexity\"],\n",
    "        \"turns\": turn,\n",
    "        \"total_messages\": total_messages,\n",
    "        \"goal_completed\": goal_completed,\n",
    "        \"conversation\": conversation,\n",
    "        \"user_message_count\": len(user_messages),\n",
    "        \"agent_message_count\": len(agent_messages)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run automated evaluation pipeline\n",
    "pipeline_results = []\n",
    "\n",
    "for idx, case in enumerate(evaluation_cases, 1):\n",
    "    print(f\"\\n[{idx}/{len(evaluation_cases)}] Evaluating: {case.name}\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    result = run_multi_turn_evaluation(case, max_turns=5)\n",
    "    pipeline_results.append(result)\n",
    "    \n",
    "    print(f\"  Category: {result['category']}\")\n",
    "    print(f\"  Complexity: {result['complexity']}\")\n",
    "    print(f\"  Turns: {result['turns']}\")\n",
    "    print(f\"  Completed: {'Yes' if result['goal_completed'] else 'No'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-29",
   "metadata": {},
   "source": [
    "### Pipeline Results Analysis\n",
    "\n",
    "Aggregate and analyze results from the automated evaluation pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate aggregate metrics\n",
    "total_cases = len(pipeline_results)\n",
    "completed_cases = sum(1 for r in pipeline_results if r['goal_completed'])\n",
    "success_rate = (completed_cases / total_cases) * 100\n",
    "avg_turns = sum(r['turns'] for r in pipeline_results) / total_cases\n",
    "avg_messages = sum(r['total_messages'] for r in pipeline_results) / total_cases\n",
    "\n",
    "# Group by complexity\n",
    "by_complexity = {}\n",
    "for result in pipeline_results:\n",
    "    complexity = result['complexity']\n",
    "    if complexity not in by_complexity:\n",
    "        by_complexity[complexity] = []\n",
    "    by_complexity[complexity].append(result)\n",
    "\n",
    "# Create results table\n",
    "results_md = f\"\"\"\n",
    "## Automated Evaluation Pipeline Results\n",
    "\n",
    "### Overall Metrics\n",
    "\n",
    "- **Total test cases**: {total_cases}\n",
    "- **Completed successfully**: {completed_cases}\n",
    "- **Success rate**: {success_rate:.1f}%\n",
    "- **Average turns per conversation**: {avg_turns:.1f}\n",
    "- **Average messages per conversation**: {avg_messages:.1f}\n",
    "\n",
    "### Results by Case\n",
    "\n",
    "| Case | Category | Complexity | Turns | Completed |\n",
    "|:-----|:---------|:-----------|:------|:----------|\n",
    "\"\"\"\n",
    "\n",
    "for result in pipeline_results:\n",
    "    completed_icon = \"Yes\" if result['goal_completed'] else \"No\"\n",
    "    results_md += f\"| {result['case_name']} | {result['category']} | {result['complexity']} | {result['turns']} | {completed_icon} |\\n\"\n",
    "\n",
    "results_md += \"\"\"\n",
    "\n",
    "### Performance by Complexity\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "for complexity, results in sorted(by_complexity.items()):\n",
    "    count = len(results)\n",
    "    completed = sum(1 for r in results if r['goal_completed'])\n",
    "    rate = (completed / count) * 100\n",
    "    avg_t = sum(r['turns'] for r in results) / count\n",
    "    \n",
    "    results_md += f\"\"\"\n",
    "**{complexity.upper()} Complexity:**\n",
    "- Cases: {count}\n",
    "- Success rate: {rate:.1f}%\n",
    "- Average turns: {avg_t:.1f}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "display(Markdown(results_md))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-31",
   "metadata": {},
   "source": "### Dev-to-Prod Workflow\n\nMove from development evaluation to production deployment with metric comparison.\n\n#### Workflow Stages\n\n| Stage | Focus | Key Activities |\n|:------|:------|:---------------|\n| Development | Small dataset, detailed logging | Comprehensive scenario coverage, manual review |\n| Staging | Scale up dataset, track metrics | Statistical analysis, anomaly detection |\n| Production | Monitor live metrics | Compare with baseline, alert on degradation |"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate development metrics (from our evaluation)\n",
    "dev_metrics = {\n",
    "    \"environment\": \"development\",\n",
    "    \"test_cases\": total_cases,\n",
    "    \"success_rate\": success_rate,\n",
    "    \"avg_turns\": avg_turns,\n",
    "    \"avg_messages\": avg_messages,\n",
    "    \"goal_completion_rate\": (completed_cases / total_cases) * 100,\n",
    "    \"error_rate\": 0.0,\n",
    "    \"dataset_size\": len(evaluation_cases)\n",
    "}\n",
    "\n",
    "# Simulate production metrics (typically collected from live system)\n",
    "prod_metrics = {\n",
    "    \"environment\": \"production\",\n",
    "    \"conversations\": 1000,\n",
    "    \"success_rate\": 92.5,\n",
    "    \"avg_turns\": 3.2,\n",
    "    \"avg_messages\": 6.8,\n",
    "    \"goal_completion_rate\": 94.0,\n",
    "    \"error_rate\": 2.5,\n",
    "    \"avg_response_time_ms\": 450,\n",
    "    \"cost_per_conversation\": 0.012\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dev-to-prod comparison\n",
    "comparison_md = \"\"\"\n",
    "## Development vs Production Metrics Comparison\n",
    "\n",
    "### Metric Comparison Table\n",
    "\n",
    "| Metric | Development | Production | Difference | Status |\n",
    "|:-------|:------------|:-----------|:-----------|:-------|\n",
    "\"\"\"\n",
    "\n",
    "# Compare common metrics\n",
    "metrics_to_compare = [\n",
    "    (\"Success Rate\", \"success_rate\", \"%\", \"higher_better\"),\n",
    "    (\"Avg Turns\", \"avg_turns\", \"turns\", \"lower_better\"),\n",
    "    (\"Avg Messages\", \"avg_messages\", \"messages\", \"lower_better\"),\n",
    "    (\"Goal Completion\", \"goal_completion_rate\", \"%\", \"higher_better\"),\n",
    "    (\"Error Rate\", \"error_rate\", \"%\", \"lower_better\")\n",
    "]\n",
    "\n",
    "for metric_name, metric_key, unit, direction in metrics_to_compare:\n",
    "    dev_val = dev_metrics.get(metric_key, 0)\n",
    "    prod_val = prod_metrics.get(metric_key, 0)\n",
    "    diff = prod_val - dev_val\n",
    "    \n",
    "    if direction == \"higher_better\":\n",
    "        status = \"Better\" if diff > 0 else \"Worse\" if diff < 0 else \"Same\"\n",
    "    else:\n",
    "        status = \"Better\" if diff < 0 else \"Worse\" if diff > 0 else \"Same\"\n",
    "    \n",
    "    comparison_md += f\"| {metric_name} | {dev_val:.1f}{unit} | {prod_val:.1f}{unit} | {diff:+.1f}{unit} | {status} |\\n\"\n",
    "\n",
    "comparison_md += f\"\"\"\n",
    "\n",
    "### Production-Only Metrics\n",
    "\n",
    "- **Average Response Time**: {prod_metrics['avg_response_time_ms']}ms\n",
    "- **Cost per Conversation**: ${prod_metrics['cost_per_conversation']:.4f}\n",
    "- **Total Conversations**: {prod_metrics['conversations']:,}\n",
    "\n",
    "### Analysis\n",
    "\n",
    "**Development Phase:**\n",
    "- Evaluated with {dev_metrics['test_cases']} carefully crafted test cases\n",
    "- Success rate: {dev_metrics['success_rate']:.1f}%\n",
    "- Comprehensive coverage of scenarios\n",
    "- Zero errors in controlled environment\n",
    "\n",
    "**Production Phase:**\n",
    "- Real users with diverse behaviors\n",
    "- Success rate: {prod_metrics['success_rate']:.1f}% (slightly lower due to edge cases)\n",
    "- Small error rate ({prod_metrics['error_rate']:.1f}%) from unexpected inputs\n",
    "- Faster conversations (fewer turns) as users learn the system\n",
    "\n",
    "**Key Insights:**\n",
    "\n",
    "1. **Success Rate**: Production rate is close to development, indicating good test coverage\n",
    "2. **Conversation Efficiency**: Production users complete tasks faster (fewer turns)\n",
    "3. **Error Handling**: Small error rate in production is expected and acceptable\n",
    "4. **Goal Completion**: High completion rate in production validates agent design\n",
    "\n",
    "**Recommendations:**\n",
    "\n",
    "1. **Monitor Production**: Continuously track metrics to detect degradation\n",
    "2. **Update Test Cases**: Add production edge cases to development dataset\n",
    "3. **Optimize Performance**: Focus on reducing response time further\n",
    "4. **Cost Management**: Monitor cost per conversation for budget planning\n",
    "5. **Error Analysis**: Investigate and fix causes of production errors\n",
    "\n",
    "### Dev-to-Prod Checklist\n",
    "\n",
    "Before deploying to production:\n",
    "\n",
    "- [ ] Success rate > 90% in development\n",
    "- [ ] All critical scenarios tested\n",
    "- [ ] Error handling implemented for common failures\n",
    "- [ ] Performance benchmarks established\n",
    "- [ ] Monitoring and alerting configured\n",
    "- [ ] Rollback plan prepared\n",
    "- [ ] Cost estimates validated\n",
    "- [ ] User feedback mechanism in place\n",
    "\"\"\"\n",
    "\n",
    "display(Markdown(comparison_md))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-34",
   "metadata": {},
   "source": "## Best Practices for Multi-turn Evaluation\n\n### When to Use ActorSimulator\n\n| Use When | Don't Use When |\n|:---------|:---------------|\n| Testing conversational agents | Single-turn Q&A is sufficient |\n| Evaluating context maintenance | Non-conversational systems |\n| Testing diverse user personas | Deterministic conversation flow |\n| Automating conversation testing at scale | |\n\n### Designing Effective Actor Profiles\n\n| Element | Good Practice |\n|:--------|:--------------|\n| Goals | Specific, measurable success criteria |\n| Traits | Realistic behaviors matching real users |\n| Context | Enough background for natural conversation |\n| Completion | Well-defined conditions for `<stop/>` token |\n\n### Scaling Evaluation\n\n| Scale | Approach |\n|:------|:---------|\n| Small (< 10 cases) | Run individually, full logging, manual review |\n| Large (100+ cases) | Batch process, aggregate metrics, sample review |\n\n### Production Monitoring\n\nTrack: success rate, average turns, completion rate, error rate, response latency, cost per conversation.\n\nAlert on: success rate drops, error spikes, turn increases, latency exceeds limits.\n\n### Common Pitfalls\n\n| Pitfall | Solution |\n|:--------|:---------|\n| Insufficient persona diversity | Include demanding, confused, edge-case personas |\n| Overly strict success criteria | Define clear, achievable goal completion |\n| Too few test cases | Aim for 20+ cases covering diverse situations |\n| Ignoring production metrics | Update tests based on production insights |"
  },
  {
   "cell_type": "markdown",
   "id": "cell-35",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You've successfully learned how to implement comprehensive multi-turn evaluation using ActorSimulator. You now understand:\n",
    "\n",
    "- **ActorSimulator API**: How to create realistic user simulations with `from_case_for_user_simulator`\n",
    "- **Actor profiles**: Designing personas with traits, context, goals, and success criteria\n",
    "- **Goal completion**: Using `<stop/>` tokens to signal when actor goals are achieved\n",
    "- **Multi-turn conversations**: Running natural conversation flows between actors and agents\n",
    "- **Diverse personas**: Testing agents with polite, demanding, and confused user types\n",
    "- **Automated pipelines**: Scaling evaluation with datasets and batch processing\n",
    "- **Dev-to-prod workflow**: Moving from development testing to production deployment\n",
    "- **Metric comparison**: Tracking and comparing development vs production performance\n",
    "- **Best practices**: When to use ActorSimulator, how to design profiles, and scaling strategies\n",
    "\n",
    "Multi-turn evaluation with ActorSimulator is essential for building robust conversational agents that handle real-world user interactions effectively. By simulating diverse user behaviors and conversation patterns, you can identify issues early, optimize agent performance, and ensure production readiness before deployment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}