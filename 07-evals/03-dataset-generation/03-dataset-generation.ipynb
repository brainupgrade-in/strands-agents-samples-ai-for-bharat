{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Generation - Automated Test Case Creation for Agent Evaluation\n",
    "\n",
    "This tutorial demonstrates automated test case generation for agent evaluation. You'll learn how to generate diverse, high-quality test datasets using the DatasetGenerator API, including saving and loading datasets for reuse across evaluation runs.\n",
    "\n",
    "### What You'll Learn\n",
    "- Generate test cases from scratch using topics\n",
    "- Generate contextual test cases from agent tools and APIs\n",
    "- Update existing datasets with edge cases and corner scenarios\n",
    "- Save datasets to JSON files for reuse\n",
    "- Load datasets from JSON files\n",
    "- Use auto-rubric generation for evaluators\n",
    "- Apply topic planning for diverse test coverage\n",
    "\n",
    "### Tutorial Details\n",
    "\n",
    "| Information         | Details                                                                       |\n",
    "|:--------------------|:------------------------------------------------------------------------------|\n",
    "| Tutorial type       | Intermediate - Automated dataset generation with persistence                  |\n",
    "| Tutorial components | Multi-agent system, DatasetGenerator, dataset persistence                     |\n",
    "| Tutorial vertical   | Agent Evaluation                                                              |\n",
    "| Example complexity  | Medium                                                                        |\n",
    "| SDK used            | Strands Agents, Strands Evals                                                 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Dataset Generation\n",
    "\n",
    "Dataset generation automates test case creation for evaluating AI agents. Instead of manually writing test cases, you use AI to generate diverse, comprehensive test scenarios.\n",
    "\n",
    "#### Why Use Dataset Generation?\n",
    "\n",
    "| Manual Creation | Automated Generation |\n",
    "|:----------------|:---------------------|\n",
    "| Time-consuming | Generate 10-100+ cases in minutes |\n",
    "| Limited coverage | Diverse topic coverage via topic planning |\n",
    "| Hard to anticipate edge cases | Automatically includes edge cases |\n",
    "| Requires domain expertise | Generates evaluation rubrics automatically |\n",
    "\n",
    "#### When to Use Dataset Generation\n",
    "\n",
    "Use dataset generation when you need comprehensive coverage across multiple scenarios, rapid prototyping during development, domain-specific testing based on agent tools, regression testing as your agent evolves, or continuous evaluation across different agent versions.\n",
    "\n",
    "#### Three Generation Strategies\n",
    "\n",
    "| Strategy | Method | Best For |\n",
    "|:---------|:-------|:---------|\n",
    "| From Scratch | `from_scratch_async()` | New agents, broad coverage, exploratory testing |\n",
    "| From Context | `from_context_async()` | Testing specific tools, API integration scenarios |\n",
    "| Update Existing | `update_current_dataset_async()` | Adding edge cases, iterative improvement |\n",
    "\n",
    "#### Dataset Persistence\n",
    "\n",
    "| Operation | Method | Use Case |\n",
    "|:----------|:-------|:---------|\n",
    "| Save | `dataset.to_file('name.json')` | Preserve for reuse, version control |\n",
    "| Load | `Dataset.from_file('name.json')` | Consistent evaluation, team sharing |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Setup\n",
    "\n",
    "Configure AWS region and model settings for this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# AWS Configuration\n",
    "session = boto3.Session()\n",
    "AWS_REGION = session.region_name or 'us-east-1'\n",
    "DEFAULT_MODEL = 'us.anthropic.claude-3-7-sonnet-20250219-v1:0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup and Imports\n",
    "\n",
    "Import all necessary libraries for agent creation, dataset generation, and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import asyncio\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "# Strands imports\n",
    "from strands import Agent, tool\n",
    "from strands.multiagent import GraphBuilder\n",
    "\n",
    "# Strands Evals imports\n",
    "from strands_evals import Dataset, Case\n",
    "from strands_evals.generators import DatasetGenerator\n",
    "from strands_evals.evaluators import OutputEvaluator\n",
    "\n",
    "# Display utilities\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Agent System with Parallel Execution\n",
    "\n",
    "We'll create a multi-agent decision-making system with parallel execution. Specialized agents analyze different aspects of a problem and feed into a final decision-maker.\n",
    "\n",
    "#### Agent Code\n",
    "\n",
    "The following multi-agent code demonstrates parallel execution with memory branching, adapted from graph agent patterns in strands-samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-agent code adapted from: /strands-samples/01-tutorials/02-multi-agent-systems/03-graph-agent/\n",
    "\n",
    "# Create specialized agents for parallel analysis\n",
    "financial_advisor = Agent(\n",
    "    name=\"financial_advisor\",\n",
    "    system_prompt=\"You are a financial advisor focused on cost-benefit analysis, budget implications, and ROI calculations. Provide concise financial assessment.\",\n",
    "    model=DEFAULT_MODEL\n",
    ")\n",
    "\n",
    "technical_architect = Agent(\n",
    "    name=\"technical_architect\",\n",
    "    system_prompt=\"You are a technical architect who evaluates feasibility, implementation challenges, and technical risks. Provide concise technical assessment.\",\n",
    "    model=DEFAULT_MODEL\n",
    ")\n",
    "\n",
    "market_researcher = Agent(\n",
    "    name=\"market_researcher\",\n",
    "    system_prompt=\"You are a market researcher who analyzes market conditions, user needs, and competitive landscape. Provide concise market assessment.\",\n",
    "    model=DEFAULT_MODEL\n",
    ")\n",
    "\n",
    "risk_analyst = Agent(\n",
    "    name=\"risk_analyst\",\n",
    "    system_prompt=\"You are a risk analyst who synthesizes input from finance, technical, and market experts to identify potential risks, mitigation strategies, and provide a final recommendation.\",\n",
    "    model=DEFAULT_MODEL\n",
    ")\n",
    "\n",
    "# Build the agent graph with parallel execution\n",
    "builder = GraphBuilder()\n",
    "\n",
    "# Add nodes\n",
    "builder.add_node(financial_advisor, \"finance_expert\")\n",
    "builder.add_node(technical_architect, \"tech_expert\")\n",
    "builder.add_node(market_researcher, \"market_expert\")\n",
    "builder.add_node(risk_analyst, \"risk_analyst\")\n",
    "\n",
    "# Add edges - parallel execution pattern\n",
    "# Finance expert feeds into both tech and market experts\n",
    "builder.add_edge(\"finance_expert\", \"tech_expert\")\n",
    "builder.add_edge(\"finance_expert\", \"market_expert\")\n",
    "\n",
    "# Both tech and market experts feed into risk analyst\n",
    "builder.add_edge(\"tech_expert\", \"risk_analyst\")\n",
    "builder.add_edge(\"market_expert\", \"risk_analyst\")\n",
    "\n",
    "# Set entry point\n",
    "builder.set_entry_point(\"finance_expert\")\n",
    "\n",
    "# Build the graph\n",
    "decision_graph = builder.build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Multi-Agent System\n",
    "\n",
    "Before generating datasets, let's verify the multi-agent system works correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the multi-agent system\n",
    "test_query = \"Should we invest $500K in developing an AI-powered customer service chatbot?\"\n",
    "result = decision_graph(test_query)\n",
    "\n",
    "# Show execution flow\n",
    "print(\"\\nExecution Order:\")\n",
    "for node in result.execution_order:\n",
    "    print(f\"  - {node.node_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy 1: Generate Dataset from Scratch\n",
    "\n",
    "The `from_scratch_async()` method generates test cases from a list of topics. This strategy is ideal when you want to ensure diverse coverage across multiple domains or scenarios.\n",
    "\n",
    "#### Key Features\n",
    "- **Topic-based generation**: Specify topics to ensure comprehensive coverage\n",
    "- **Auto-rubric generation**: Automatically creates evaluation rubrics\n",
    "- **Difficulty distribution**: Generates easy, medium, and hard test cases\n",
    "- **Dataset persistence**: Save to JSON for reuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dataset generator\n",
    "generator = DatasetGenerator(\n",
    "    input_type=str,\n",
    "    output_type=str,\n",
    "    include_expected_output=True,\n",
    "    model=DEFAULT_MODEL\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dataset from scratch with topics\n",
    "topics = [\n",
    "    \"technology investments\",\n",
    "    \"business process automation\",\n",
    "    \"market expansion strategies\"\n",
    "]\n",
    "\n",
    "# Generate dataset\n",
    "scratch_dataset = await generator.from_scratch_async(\n",
    "    topics=topics,\n",
    "    task_description=\"Multi-agent decision system that provides recommendations on business investments and strategies\",\n",
    "    num_cases=9,\n",
    "    evaluator=OutputEvaluator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Dataset to JSON\n",
    "\n",
    "Save the generated dataset to a JSON file for reuse in future evaluation runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataset to JSON file\n",
    "scratch_dataset.to_file('scratch_dataset.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preview Generated Test Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample test cases\n",
    "for i, case in enumerate(scratch_dataset.cases[:3], 1):\n",
    "    case_info = f\"\"\"\n",
    "**Case {i}: {case.name}**\n",
    "**Input**: {case.input}\n",
    "**Expected Output**: {case.expected_output}\n",
    "    \"\"\"\n",
    "    display(Markdown(case_info))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy 2: Generate Dataset from Context\n",
    "\n",
    "The `from_context_async()` method generates test cases based on your agent's specific context, such as tool definitions, APIs, or documentation. This ensures test cases are relevant to your agent's actual capabilities.\n",
    "\n",
    "#### Key Features\n",
    "- **Context-aware generation**: Uses agent tools and APIs to create relevant tests\n",
    "- **Topic planning**: Optionally expand context into diverse topics\n",
    "- **Tool-specific testing**: Generates tests that exercise specific tools\n",
    "- **Auto-rubric generation**: Creates rubrics aligned with context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define agent context (tools and capabilities)\n",
    "agent_context = \"\"\"\n",
    "Multi-agent decision system with the following capabilities:\n",
    "\n",
    "Agents:\n",
    "- Financial Advisor: Analyzes costs, ROI, budget impact, financial risks\n",
    "- Technical Architect: Evaluates technical feasibility, implementation complexity, architecture risks\n",
    "- Market Researcher: Assesses market demand, competition, user needs, market timing\n",
    "- Risk Analyst: Synthesizes all inputs to provide final recommendation\n",
    "\n",
    "Decision Flow:\n",
    "1. Financial analysis runs first\n",
    "2. Technical and market analysis run in parallel\n",
    "3. Risk analyst synthesizes all perspectives\n",
    "\n",
    "Output Format:\n",
    "- Financial assessment\n",
    "- Technical assessment\n",
    "- Market assessment\n",
    "- Final risk analysis and recommendation\n",
    "\"\"\"\n",
    "\n",
    "# Generate dataset with topic planning for diversity\n",
    "context_dataset = await generator.from_context_async(\n",
    "    context=agent_context,\n",
    "    task_description=\"Multi-agent system that evaluates business decisions across financial, technical, and market dimensions\",\n",
    "    num_cases=12,\n",
    "    num_topics=4,  # Topic planning: expand into 4 diverse topics\n",
    "    evaluator=OutputEvaluator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Context-Based Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save context-based dataset to JSON\n",
    "context_dataset.to_file('context_dataset.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preview Context-Based Test Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample test cases\n",
    "for i, case in enumerate(context_dataset.cases[:3], 1):\n",
    "    case_info = f\"\"\"\n",
    "**Case {i}: {case.name}**\n",
    "**Input**: {case.input}\n",
    "**Expected Output**: {case.expected_output[:200]}...\n",
    "    \"\"\"\n",
    "    display(Markdown(case_info))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy 3: Update Existing Dataset with Edge Cases\n",
    "\n",
    "The `update_current_dataset_async()` method extends an existing dataset by adding new test cases. This is ideal for iteratively improving test coverage by adding edge cases, corner scenarios, or addressing gaps discovered in production.\n",
    "\n",
    "#### Key Features\n",
    "- **Incremental improvement**: Add tests without starting from scratch\n",
    "- **Edge case coverage**: Focus on corner cases and failure scenarios\n",
    "- **Dataset continuity**: Preserves existing tests while adding new ones\n",
    "- **Rubric updates**: Optionally update evaluation rubrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Existing Dataset from JSON\n",
    "\n",
    "First, let's load one of our previously saved datasets to demonstrate the update workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load existing dataset from JSON\n",
    "loaded_dataset = Dataset.from_file('scratch_dataset.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update dataset with edge cases\n",
    "edge_case_context = \"\"\"\n",
    "Add edge cases and challenging scenarios:\n",
    "- Conflicting financial and technical assessments\n",
    "- High-risk, high-reward decisions\n",
    "- Decisions with missing or incomplete information\n",
    "- Time-sensitive decisions requiring rapid analysis\n",
    "- Decisions involving ethical considerations\n",
    "- Scenarios where experts disagree\n",
    "\"\"\"\n",
    "\n",
    "# Update dataset by adding edge cases\n",
    "updated_dataset = await generator.update_current_dataset_async(\n",
    "    source_dataset=loaded_dataset,\n",
    "    task_description=\"Multi-agent decision system handling complex and edge case scenarios\",\n",
    "    num_cases=6,\n",
    "    context=edge_case_context,\n",
    "    add_new_cases=True\n",
    ")\n",
    "\n",
    "print(f\"\\nOriginal dataset: {len(loaded_dataset.cases)} cases\")\n",
    "print(f\"Updated dataset: {len(updated_dataset.cases)} cases\")\n",
    "print(f\"New cases added: {len(updated_dataset.cases) - len(loaded_dataset.cases)}\")\n",
    "print(f\"\\nUpdated rubric: {updated_dataset.evaluator.rubric}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Updated Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save updated dataset to JSON\n",
    "print(f\"Total test cases: {len(updated_dataset.cases)}\")\n",
    "updated_dataset.to_file('updated_dataset.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preview Edge Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display newly added edge cases (last 3 cases)\n",
    "new_cases = updated_dataset.cases[-3:]\n",
    "for i, case in enumerate(new_cases, 1):\n",
    "    case_info = f\"\"\"\n",
    "**Edge Case {i}: {case.name}**\n",
    "**Input**: {case.input}\n",
    "**Expected Output**: {case.expected_output[:200]}...\n",
    "    \"\"\"\n",
    "    display(Markdown(case_info))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Evaluation with Generated Dataset\n",
    "\n",
    "Now let's evaluate our multi-agent system using one of the generated datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define agent task function\n",
    "def agent_task(case: Case) -> str:\n",
    "    \"\"\"\n",
    "    Execute the multi-agent decision system with the given case input.\n",
    "    \"\"\"\n",
    "    result = decision_graph(case.input)\n",
    "    return str(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use first 3 cases for demonstration\n",
    "eval_dataset = Dataset(\n",
    "    cases=context_dataset.cases[:3],\n",
    "    evaluator=context_dataset.evaluator\n",
    ")\n",
    "\n",
    "report = eval_dataset.run_evaluations(agent_task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Results\n",
    "\n",
    "Display the evaluation results using the auto-generated rubric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display evaluation report\n",
    "report.run_display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Persistence Workflow\n",
    "\n",
    "Let's demonstrate a complete workflow showing how datasets can be saved and loaded across different evaluation sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of dataset persistence workflow\n",
    "workflow_summary = \"\"\"\n",
    "## Dataset Persistence Workflow Summary\n",
    "\n",
    "### Generated Datasets\n",
    "\n",
    "**1. scratch_dataset.json**\n",
    "- Strategy: from_scratch_async()\n",
    "- Topics: technology investments, business automation, market expansion\n",
    "- Test cases: 9\n",
    "- Use case: Broad coverage testing\n",
    "\n",
    "**2. context_dataset.json**\n",
    "- Strategy: from_context_async()\n",
    "- Context: Multi-agent capabilities and decision flow\n",
    "- Test cases: 12 (with topic planning)\n",
    "- Use case: Context-aware testing\n",
    "\n",
    "**3. updated_dataset.json**\n",
    "- Strategy: update_current_dataset_async()\n",
    "- Source: scratch_dataset.json + edge cases\n",
    "- Test cases: 15 (original 9 + 6 new)\n",
    "- Use case: Iterative improvement with edge cases\n",
    "\n",
    "### Loading Datasets\n",
    "\n",
    "```python\n",
    "# Load any saved dataset\n",
    "dataset = Dataset.from_file('dataset_name.json')\n",
    "\n",
    "# Run evaluation\n",
    "report = dataset.run_evaluations(agent_task)\n",
    "```\n",
    "\n",
    "### Benefits\n",
    "\n",
    "- **Consistency**: Use the same test suite across agent versions\n",
    "- **Collaboration**: Share datasets with team members\n",
    "- **Version Control**: Track dataset changes over time\n",
    "- **Regression Testing**: Ensure new changes don't break existing functionality\n",
    "- **CI/CD Integration**: Automate evaluation in deployment pipelines\n",
    "\"\"\"\n",
    "\n",
    "display(Markdown(workflow_summary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices for Dataset Generation\n",
    "\n",
    "### Choosing the Right Strategy\n",
    "\n",
    "| Strategy | Use When |\n",
    "|:---------|:---------|\n",
    "| `from_scratch_async()` | Starting new project, need broad coverage, no detailed context yet |\n",
    "| `from_context_async()` | Have well-defined tools/APIs, need tests matching actual capabilities |\n",
    "| `update_current_dataset_async()` | Improving existing dataset, discovered gaps, adding edge cases |\n",
    "\n",
    "### Key Recommendations\n",
    "\n",
    "| Area | Recommendation |\n",
    "|:-----|:---------------|\n",
    "| Topic Planning | Use `num_topics=3-6` for diverse coverage |\n",
    "| Persistence | Save datasets when generation takes time or needs reuse |\n",
    "| Auto-rubrics | Best with default evaluators; use manual rubrics for specific requirements |\n",
    "| Iteration | Start broad \u2192 add context \u2192 refine with edge cases \u2192 evaluate \u2192 iterate |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You've successfully learned how to generate and persist evaluation datasets using Strands Evals. You now understand:\n",
    "\n",
    "- How to generate test cases from scratch using topics with `from_scratch_async()`\n",
    "- How to generate contextual test cases from agent capabilities with `from_context_async()`\n",
    "- How to update existing datasets with edge cases using `update_current_dataset_async()`\n",
    "- How to save datasets to JSON files with `dataset.to_file()`\n",
    "- How to load datasets from JSON files with `Dataset.from_file()`\n",
    "- How to use auto-rubric generation for evaluators\n",
    "- How to apply topic planning for diverse test coverage\n",
    "- Best practices for choosing generation strategies\n",
    "\n",
    "Dataset generation enables you to create comprehensive, diverse test suites quickly, and dataset persistence ensures you can reuse these tests consistently across evaluation runs. This forms the foundation for robust, continuous agent evaluation workflows."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}