{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial 02: Custom Evaluators - Building Domain-Specific Evaluation Criteria\n",
    "\n",
    "While built-in evaluators provide general-purpose evaluation capabilities, custom evaluators allow you to define domain-specific evaluation criteria tailored to your agent's unique requirements. In this tutorial, you'll learn how to create custom evaluators with well-defined rubrics to assess recipe quality, dietary compliance, food safety, and overall recipe helpfulness using LLM-as-a-judge.\n",
    "\n",
    "### What You'll Learn\n",
    "- How to extend the base Evaluator class to create custom evaluators\n",
    "- Design rubrics with clear scoring criteria (3-point and 5-point scales)\n",
    "- Create domain-specific evaluators for recipe quality, dietary compliance, and food safety\n",
    "- Implement an LLM-as-a-judge custom evaluator with a 5-point scale (1-5)\n",
    "- Combine multiple custom evaluators in a single evaluation workflow\n",
    "- Understand when to use custom evaluators versus built-in evaluators\n",
    "\n",
    "### Tutorial Details\n",
    "\n",
    "| Information         | Details                                                                       |\n",
    "|:--------------------|:------------------------------------------------------------------------------|\n",
    "| Tutorial type       | Beginner - Introduction to creating custom evaluation criteria                |\n",
    "| Tutorial components | Custom evaluators, rubric design, LLM-as-judge, multi-metric evaluation      |\n",
    "| Tutorial vertical   | Agent Evaluation                                                              |\n",
    "| Example complexity  | Easy                                                                          |\n",
    "| SDK used            | Strands Agents, Strands Evals                                                 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Custom Evaluators\n",
    "\n",
    "Custom evaluators extend the base `Evaluator` class to define domain-specific criteria beyond general-purpose metrics. Use them when you need specialized quality standards, business rules, or compliance requirements.\n",
    "\n",
    "#### When to Use Custom Evaluators\n",
    "\n",
    "| Use Case | Description |\n",
    "|:---------|:------------|\n",
    "| Domain-Specific Requirements | Criteria unique to your domain (recipe completeness, medical accuracy, legal compliance) |\n",
    "| Business Logic Validation | Agent outputs must meet specific business rules |\n",
    "| Specialized Quality Metrics | Standard metrics don't capture your use case nuances |\n",
    "| LLM-as-Judge | Sophisticated evaluation requiring context and nuance |\n",
    "\n",
    "#### Key Components\n",
    "\n",
    "| Component | Purpose |\n",
    "|:----------|:--------|\n",
    "| Rubric | Clear scoring criteria with defined thresholds |\n",
    "| Evaluation Logic | Code that analyzes agent output against the rubric |\n",
    "| Structured Results | Score, label, and explanation of the evaluation |\n",
    "\n",
    "#### Rubric Scale Patterns\n",
    "\n",
    "| Scale | Scores | Best For |\n",
    "|:------|:-------|:---------|\n",
    "| 3-Point | 0 (Not met), 1 (Partial), 2 (Full) | Simple pass/fail criteria |\n",
    "| 5-Point (0-4) | 0 (Absent) to 4 (Complete) | Detailed quality assessment |\n",
    "| 5-Point (1-5) | 1 (Very Poor) to 5 (Excellent) | LLM-as-Judge evaluations |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Setup\n",
    "\n",
    "Configure AWS region and model settings using inline configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWS Configuration (inline - no config.py)\n",
    "import boto3\n",
    "\n",
    "session = boto3.Session()\n",
    "AWS_REGION = session.region_name or 'us-east-1'\n",
    "DEFAULT_MODEL = 'us.anthropic.claude-3-7-sonnet-20250219-v1:0'\n",
    "JUDGE_MODEL = 'us.anthropic.claude-3-7-sonnet-20250219-v1:0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup and Imports\n",
    "\n",
    "Import all required packages for agent creation and custom evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import json\n",
    "from typing import Any\n",
    "\n",
    "# Strands imports\n",
    "from strands import Agent, tool\n",
    "from strands_evals import Dataset, Case\n",
    "from strands_evals.evaluators import Evaluator\n",
    "from strands_evals.types import EvaluationData, EvaluationOutput\n",
    "\n",
    "# Display utilities\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Web search tool dependency\n",
    "from ddgs import DDGS\n",
    "from ddgs.exceptions import DDGSException, RatelimitException"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recipe Bot Agent\n",
    "\n",
    "We'll use the Recipe Bot agent from Tutorial 01 as our evaluation target. This agent helps users find recipes and answer cooking questions using web search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a websearch tool\n",
    "@tool\n",
    "def websearch(\n",
    "    keywords: str, region: str = \"us-en\", max_results: int | None = None\n",
    ") -> str:\n",
    "    \"\"\"Search the web to get updated information.\n",
    "    Args:\n",
    "        keywords (str): The search query keywords.\n",
    "        region (str): The search region: wt-wt, us-en, uk-en, ru-ru, etc..\n",
    "        max_results (int | None): The maximum number of results to return.\n",
    "    Returns:\n",
    "        List of dictionaries with search results.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        results = DDGS().text(keywords, region=region, max_results=max_results)\n",
    "        return results if results else \"No results found.\"\n",
    "    except RatelimitException:\n",
    "        return \"RatelimitException: Please try again after a short delay.\"\n",
    "    except DDGSException as d:\n",
    "        return f\"DuckDuckGoSearchException: {d}\"\n",
    "    except Exception as e:\n",
    "        return f\"Exception: {e}\"\n",
    "\n",
    "\n",
    "# Create a recipe assistant agent\n",
    "recipe_agent = Agent(\n",
    "    model=DEFAULT_MODEL,\n",
    "    system_prompt=\"\"\"You are RecipeBot, a helpful cooking assistant.\n",
    "    Help users find recipes based on ingredients and answer cooking questions.\n",
    "    Use the websearch tool to find recipes when users mention ingredients or to look up cooking information.\"\"\",\n",
    "    tools=[websearch],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Agent\n",
    "\n",
    "Before creating custom evaluators, let's verify the agent works correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the agent with a simple query\n",
    "recipe_agent(\"Give me a simple pasta recipe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Custom Evaluators\n",
    "\n",
    "We'll create four custom evaluators, each focusing on a different aspect of recipe quality:\n",
    "\n",
    "| Evaluator | Scale | What It Measures |\n",
    "|:----------|:------|:-----------------|\n",
    "| RecipeQualityEvaluator | 5-point (0-4) | Ingredients, steps, timing completeness |\n",
    "| DietaryComplianceEvaluator | 3-point | Respects dietary restrictions |\n",
    "| RecipeSafetyEvaluator | 3-point | Food safety information |\n",
    "| RecipeHelpfulnessLLMJudge | 5-point (1-5) | Overall helpfulness via LLM |\n",
    "\n",
    "#### Custom Evaluator 1: RecipeQualityEvaluator\n",
    "\n",
    "Assesses whether a recipe includes essential components: ingredients list, preparation steps, and timing information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecipeQualityEvaluator(Evaluator[str, str]):\n",
    "    \"\"\"Evaluates recipe completeness: ingredients, steps, and timing.\n",
    "    \n",
    "    Rubric (5-point scale: 0-4):\n",
    "    - 0: No recipe components present\n",
    "    - 1: Only one component present (e.g., just ingredients)\n",
    "    - 2: Two components present (e.g., ingredients and steps, but no timing)\n",
    "    - 3: All three components present but incomplete\n",
    "    - 4: All three components present and complete\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def evaluate(self, evaluation_case: EvaluationData[str, str]) -> list[EvaluationOutput]:\n",
    "        \"\"\"Evaluate recipe quality based on completeness.\"\"\"\n",
    "        actual = evaluation_case.actual_output or \"\"\n",
    "        output_text = str(actual).lower()\n",
    "        \n",
    "        has_ingredients = any(keyword in output_text for keyword in \n",
    "                            ['ingredient', 'cup', 'tablespoon', 'teaspoon', 'oz', 'gram'])\n",
    "        has_steps = any(keyword in output_text for keyword in \n",
    "                       ['step', 'instruction', 'cook', 'mix', 'add', 'prepare', 'heat'])\n",
    "        has_timing = any(keyword in output_text for keyword in \n",
    "                        ['minute', 'hour', 'time', 'until', 'second'])\n",
    "        \n",
    "        components_count = sum([has_ingredients, has_steps, has_timing])\n",
    "        \n",
    "        if components_count == 0:\n",
    "            score = 0\n",
    "            label = \"Incomplete\"\n",
    "            explanation = \"Recipe lacks all essential components (ingredients, steps, timing)\"\n",
    "        elif components_count == 1:\n",
    "            score = 1\n",
    "            label = \"Poor\"\n",
    "            missing = []\n",
    "            if not has_ingredients: missing.append(\"ingredients\")\n",
    "            if not has_steps: missing.append(\"steps\")\n",
    "            if not has_timing: missing.append(\"timing\")\n",
    "            explanation = f\"Recipe is missing: {', '.join(missing)}\"\n",
    "        elif components_count == 2:\n",
    "            score = 2\n",
    "            label = \"Fair\"\n",
    "            missing = []\n",
    "            if not has_ingredients: missing.append(\"ingredients\")\n",
    "            if not has_steps: missing.append(\"steps\")\n",
    "            if not has_timing: missing.append(\"timing\")\n",
    "            explanation = f\"Recipe is missing: {', '.join(missing)}\"\n",
    "        else:\n",
    "            is_detailed = len(output_text) > 200\n",
    "            if is_detailed:\n",
    "                score = 4\n",
    "                label = \"Excellent\"\n",
    "                explanation = \"Recipe includes all essential components with good detail\"\n",
    "            else:\n",
    "                score = 3\n",
    "                label = \"Good\"\n",
    "                explanation = \"Recipe includes all components but could be more detailed\"\n",
    "        \n",
    "        return [EvaluationOutput(\n",
    "            score=score,\n",
    "            test_pass=score >= 3,\n",
    "            reason=explanation\n",
    "        )]\n",
    "    \n",
    "    async def evaluate_async(self, evaluation_case: EvaluationData[str, str]) -> list[EvaluationOutput]:\n",
    "        return self.evaluate(evaluation_case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom Evaluator 2: DietaryComplianceEvaluator\n",
    "\n",
    "This evaluator checks whether the recipe appropriately addresses dietary restrictions mentioned in the user's request (vegan, gluten-free, dairy-free, etc.). It uses a **3-point scale** for simplicity. This is particularly important for applications where dietary compliance could have health or ethical implications for users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DietaryComplianceEvaluator(Evaluator[str, str]):\n",
    "    \"\"\"Evaluates whether recipe respects dietary restrictions.\n",
    "    \n",
    "    Rubric (3-point scale):\n",
    "    - 0: Violates stated dietary restrictions\n",
    "    - 1: Partially compliant or unclear\n",
    "    - 2: Fully compliant with dietary restrictions\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def evaluate(self, evaluation_case: EvaluationData[str, str]) -> list[EvaluationOutput]:\n",
    "        \"\"\"Evaluate dietary compliance based on user request.\"\"\"\n",
    "        input_text = str(evaluation_case.input).lower()\n",
    "        actual = evaluation_case.actual_output or \"\"\n",
    "        output_text = str(actual).lower()\n",
    "        \n",
    "        restrictions = {\n",
    "            'vegan': ['meat', 'chicken', 'beef', 'pork', 'fish', 'egg', 'dairy', 'milk', 'cheese', 'butter'],\n",
    "            'vegetarian': ['meat', 'chicken', 'beef', 'pork', 'fish'],\n",
    "            'gluten-free': ['flour', 'wheat', 'bread', 'pasta', 'gluten'],\n",
    "            'dairy-free': ['milk', 'cheese', 'butter', 'cream', 'yogurt'],\n",
    "            'nut-free': ['nut', 'almond', 'peanut', 'walnut', 'pecan', 'cashew']\n",
    "        }\n",
    "        \n",
    "        mentioned_restrictions = [key for key in restrictions.keys() if key in input_text]\n",
    "        \n",
    "        if not mentioned_restrictions:\n",
    "            return [EvaluationOutput(\n",
    "                score=2,\n",
    "                test_pass=True,\n",
    "                reason=\"No dietary restrictions specified in the request\"\n",
    "            )]\n",
    "        \n",
    "        violations = []\n",
    "        for restriction in mentioned_restrictions:\n",
    "            for ingredient in restrictions[restriction]:\n",
    "                if ingredient in output_text:\n",
    "                    violations.append(f\"{ingredient} (violates {restriction})\")\n",
    "        \n",
    "        if violations:\n",
    "            score = 0\n",
    "            label = \"Non-Compliant\"\n",
    "            explanation = f\"Recipe violates {', '.join(mentioned_restrictions)} restrictions. Found: {', '.join(violations[:3])}\"\n",
    "        elif any(f\"{r} option\" in output_text or f\"{r} substitute\" in output_text for r in mentioned_restrictions):\n",
    "            score = 2\n",
    "            label = \"Compliant\"\n",
    "            explanation = f\"Recipe explicitly addresses {', '.join(mentioned_restrictions)} requirements\"\n",
    "        else:\n",
    "            score = 1\n",
    "            label = \"Unclear\"\n",
    "            explanation = f\"Recipe may be {', '.join(mentioned_restrictions)} but doesn't explicitly confirm\"\n",
    "        \n",
    "        return [EvaluationOutput(\n",
    "            score=score,\n",
    "            test_pass=score >= 2,\n",
    "            reason=explanation\n",
    "        )]\n",
    "    \n",
    "    async def evaluate_async(self, evaluation_case: EvaluationData[str, str]) -> list[EvaluationOutput]:\n",
    "        return self.evaluate(evaluation_case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom Evaluator 3: RecipeSafetyEvaluator\n",
    "\n",
    "This evaluator validates that recipes include important food safety information such as cooking temperatures, handling instructions, and cross-contamination warnings. It uses a **3-point scale**. Food safety is critical for preventing foodborne illness, making this evaluator essential for any recipe-based agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecipeSafetyEvaluator(Evaluator[str, str]):\n",
    "    \"\"\"Evaluates food safety considerations in recipes.\n",
    "    \n",
    "    Rubric (3-point scale):\n",
    "    - 0: No safety information present\n",
    "    - 1: Some safety information but incomplete\n",
    "    - 2: Adequate safety information included\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def evaluate(self, evaluation_case: EvaluationData[str, str]) -> list[EvaluationOutput]:\n",
    "        \"\"\"Evaluate food safety information.\"\"\"\n",
    "        actual = evaluation_case.actual_output or \"\"\n",
    "        output_text = str(actual).lower()\n",
    "        \n",
    "        has_temperature = any(keyword in output_text for keyword in \n",
    "                            ['degree', 'temperature', '\u00b0f', '\u00b0c', 'fahrenheit', 'celsius', 'internal temp'])\n",
    "        has_doneness = any(keyword in output_text for keyword in \n",
    "                          ['done', 'cooked through', 'no longer pink', 'until tender', 'golden brown'])\n",
    "        has_handling = any(keyword in output_text for keyword in \n",
    "                          ['wash', 'clean', 'sanitize', 'separate', 'refrigerat', 'thaw', 'defrost'])\n",
    "        has_storage = any(keyword in output_text for keyword in \n",
    "                         ['store', 'keep', 'leftover', 'refrigerat', 'freeze'])\n",
    "        \n",
    "        safety_indicators = sum([has_temperature, has_doneness, has_handling, has_storage])\n",
    "        \n",
    "        if safety_indicators == 0:\n",
    "            score = 0\n",
    "            label = \"Unsafe\"\n",
    "            explanation = \"Recipe lacks food safety information (temperature, doneness, handling)\"\n",
    "        elif safety_indicators == 1:\n",
    "            score = 1\n",
    "            label = \"Minimal Safety\"\n",
    "            indicators = []\n",
    "            if has_temperature: indicators.append(\"temperature\")\n",
    "            if has_doneness: indicators.append(\"doneness\")\n",
    "            if has_handling: indicators.append(\"handling\")\n",
    "            if has_storage: indicators.append(\"storage\")\n",
    "            explanation = f\"Recipe includes limited safety info: {', '.join(indicators)}\"\n",
    "        else:\n",
    "            score = 2\n",
    "            label = \"Safe\"\n",
    "            indicators = []\n",
    "            if has_temperature: indicators.append(\"temperature\")\n",
    "            if has_doneness: indicators.append(\"doneness\")\n",
    "            if has_handling: indicators.append(\"handling\")\n",
    "            if has_storage: indicators.append(\"storage\")\n",
    "            explanation = f\"Recipe includes adequate safety information: {', '.join(indicators)}\"\n",
    "        \n",
    "        return [EvaluationOutput(\n",
    "            score=score,\n",
    "            test_pass=score >= 2,\n",
    "            reason=explanation\n",
    "        )]\n",
    "    \n",
    "    async def evaluate_async(self, evaluation_case: EvaluationData[str, str]) -> list[EvaluationOutput]:\n",
    "        return self.evaluate(evaluation_case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom Evaluator 4: RecipeHelpfulnessLLMJudge\n",
    "\n",
    "This evaluator uses an LLM to judge the overall helpfulness and quality of the recipe response. Unlike rule-based evaluators, this evaluator leverages a language model's understanding to assess nuanced qualities like clarity, completeness, and practical value. It uses a **5-point scale (1-5)**.\n",
    "\n",
    "**Key Difference**: LLM-as-a-judge evaluators can understand context, detect subtle quality issues, and provide more nuanced assessments than simple keyword matching. This makes them ideal for evaluating subjective qualities that are difficult to capture with rules alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecipeHelpfulnessLLMJudge(Evaluator[str, str]):\n",
    "    \"\"\"Uses an LLM to evaluate recipe helpfulness and overall quality.\n",
    "    \n",
    "    Rubric (5-point scale: 1-5):\n",
    "    - 1: Very poor - Fails to meet basic requirements, unhelpful or incorrect\n",
    "    - 2: Poor - Meets minimal requirements but has major issues (missing key info, unclear)\n",
    "    - 3: Fair - Acceptable but room for improvement (basic recipe, lacks detail)\n",
    "    - 4: Good - Meets requirements well with minor issues (clear, complete, useful)\n",
    "    - 5: Excellent - Exceeds requirements, comprehensive, clear, and highly practical\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model: str = None):\n",
    "        super().__init__()\n",
    "        self.judge_agent = Agent(\n",
    "            model=model or JUDGE_MODEL,\n",
    "            system_prompt=\"\"\"You are an expert culinary evaluator. Evaluate recipe responses on a scale of 1-5.\n",
    "            \n",
    "            Scoring criteria:\n",
    "            - 1: Very poor - Fails to meet basic requirements, unhelpful or incorrect\n",
    "            - 2: Poor - Meets minimal requirements but has major issues\n",
    "            - 3: Fair - Acceptable but room for improvement\n",
    "            - 4: Good - Meets requirements well with minor issues\n",
    "            - 5: Excellent - Exceeds requirements, comprehensive and clear\n",
    "            \n",
    "            Consider:\n",
    "            - Completeness: Does it include ingredients, steps, timing?\n",
    "            - Clarity: Are instructions clear and easy to follow?\n",
    "            - Practicality: Can a home cook actually make this?\n",
    "            - Relevance: Does it address the user's request?\n",
    "            - Detail: Is there enough information without overwhelming?\n",
    "            \n",
    "            Respond with ONLY a JSON object in this exact format:\n",
    "            {\"score\": <1-5>, \"label\": \"<Very Poor|Poor|Fair|Good|Excellent>\", \"explanation\": \"<brief explanation>\"}\n",
    "            \"\"\"\n",
    "        )\n",
    "    \n",
    "    def evaluate(self, evaluation_case: EvaluationData[str, str]) -> list[EvaluationOutput]:\n",
    "        \"\"\"Use LLM to evaluate recipe helpfulness.\"\"\"\n",
    "        actual = evaluation_case.actual_output or \"\"\n",
    "        \n",
    "        eval_prompt = f\"\"\"Evaluate this recipe response:\n",
    "\n",
    "User Request: {evaluation_case.input}\n",
    "\n",
    "Recipe Response: {actual}\n",
    "\n",
    "Provide your evaluation as a JSON object.\"\"\"\n",
    "        \n",
    "        try:\n",
    "            judge_response = self.judge_agent(eval_prompt)\n",
    "            \n",
    "            judge_output = str(judge_response)\n",
    "            \n",
    "            if \"```json\" in judge_output:\n",
    "                json_str = judge_output.split(\"```json\")[1].split(\"```\")[0].strip()\n",
    "            elif \"```\" in judge_output:\n",
    "                json_str = judge_output.split(\"```\")[1].split(\"```\")[0].strip()\n",
    "            else:\n",
    "                json_str = judge_output.strip()\n",
    "            \n",
    "            result_data = json.loads(json_str)\n",
    "            \n",
    "            score = result_data.get(\"score\", 3)\n",
    "            label = result_data.get(\"label\", \"Fair\")\n",
    "            explanation = result_data.get(\"explanation\", \"LLM evaluation completed\")\n",
    "            \n",
    "            score = max(1, min(5, int(score)))\n",
    "            \n",
    "        except Exception as e:\n",
    "            score = 3\n",
    "            label = \"Fair\"\n",
    "            explanation = f\"LLM evaluation failed: {str(e)[:100]}\"\n",
    "        \n",
    "        return [EvaluationOutput(\n",
    "            score=score,\n",
    "            test_pass=score >= 3,\n",
    "            reason=explanation\n",
    "        )]\n",
    "    \n",
    "    async def evaluate_async(self, evaluation_case: EvaluationData[str, str]) -> list[EvaluationOutput]:\n",
    "        return self.evaluate(evaluation_case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rule-Based vs LLM-as-Judge**: Rule-based evaluators use keyword matching (fast, deterministic, objective). LLM-as-judge uses language models for subjective qualities (clarity, helpfulness) but is slower and costs more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Test Cases\n",
    "\n",
    "Now we'll create test cases that exercise different aspects of our custom evaluators. We'll use a balanced set of 3 examples covering different dietary needs and safety considerations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test cases for evaluation\n",
    "test_cases = [\n",
    "    Case(\n",
    "        name=\"Basic Recipe Request\",\n",
    "        input=\"Give me a simple chicken pasta recipe\",\n",
    "        expected_output=\"A complete recipe with ingredients, steps, and cooking times\"\n",
    "    ),\n",
    "    Case(\n",
    "        name=\"Vegan Recipe Request\",\n",
    "        input=\"I need a vegan chocolate cake recipe\",\n",
    "        expected_output=\"A vegan recipe with no animal products\"\n",
    "    ),\n",
    "    Case(\n",
    "        name=\"Food Safety Critical Recipe\",\n",
    "        input=\"How do I cook chicken safely?\",\n",
    "        expected_output=\"Recipe with proper temperature and safety guidelines\"\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataset with Custom Evaluators\n",
    "\n",
    "We'll create a dataset that uses all four custom evaluators (including the LLM-as-a-judge) to provide comprehensive, multi-metric evaluation of recipe responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize all custom evaluators\n",
    "recipe_quality_eval = RecipeQualityEvaluator()\n",
    "dietary_compliance_eval = DietaryComplianceEvaluator()\n",
    "recipe_safety_eval = RecipeSafetyEvaluator()\n",
    "llm_judge_eval = RecipeHelpfulnessLLMJudge()\n",
    "\n",
    "# Create separate datasets for each evaluator\n",
    "dataset_quality = Dataset(\n",
    "    cases=test_cases,\n",
    "    evaluator=recipe_quality_eval\n",
    ")\n",
    "\n",
    "dataset_compliance = Dataset(\n",
    "    cases=test_cases,\n",
    "    evaluator=dietary_compliance_eval\n",
    ")\n",
    "\n",
    "dataset_safety = Dataset(\n",
    "    cases=test_cases,\n",
    "    evaluator=recipe_safety_eval\n",
    ")\n",
    "\n",
    "dataset_llm_judge = Dataset(\n",
    "    cases=test_cases,\n",
    "    evaluator=llm_judge_eval\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Evaluation\n",
    "\n",
    "Execute the evaluation using our custom evaluators. Each test case will be evaluated against all four custom metrics, including the LLM-as-a-judge evaluator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define agent task wrapper\n",
    "def agent_task(case: Case) -> str:\n",
    "    \"\"\"Execute agent and return response.\"\"\"\n",
    "    response = recipe_agent(case.input)\n",
    "    return str(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Summary Report\n",
    "\n",
    "Use the built-in display functionality to view aggregated results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run separate evaluations for each evaluator\n",
    "report_quality = dataset_quality.run_evaluations(agent_task)\n",
    "# display(Markdown(\"## Recipe Quality Evaluator Report\"))\n",
    "report_quality.run_display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_quality.to_file('report')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_quality.run_display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_compliance = dataset_compliance.run_evaluations(agent_task)\n",
    "display(Markdown(\"## Dietary Compliance Evaluator Report\"))\n",
    "report_compliance.run_display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_safety = dataset_safety.run_evaluations(agent_task)\n",
    "display(Markdown(\"## Recipe Safety Evaluator Report\"))\n",
    "report_safety.run_display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluator Selection Guide\n",
    "\n",
    "**Custom**: Domain-specific criteria, business rules, specialized metrics. Use rule-based for objective criteria; LLM-as-judge for subjective.  \n",
    "**Built-In**: General capabilities, standardized metrics, baseline evaluation.  \n",
    "**Best Practice**: Combine both for comprehensive coverage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "You've learned to create custom evaluators by extending the Evaluator class, designing rubrics with 3-point and 5-point scales, and implementing both rule-based evaluators (objective criteria) and LLM-as-judge evaluators (subjective assessment)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai-on-aws",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}