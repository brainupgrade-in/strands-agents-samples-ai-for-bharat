{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AB Testing Models - Evaluating Agent Performance\n",
    "\n",
    "Learn how to systematically evaluate multiple agent variants using different language models. This tutorial shows you how to run parallel evaluations, compare performance metrics, analyze cost-performance trade-offs, and generate data-driven model selection recommendations.\n",
    "\n",
    "### What You'll Learn\n",
    "- Load agent configurations from Part 1 and recreate agents\n",
    "- Create diverse evaluation datasets from TauBench scenarios\n",
    "- Run systematic evaluations on multiple agent variants\n",
    "- Use five built-in evaluators for comprehensive assessment\n",
    "- Compare evaluation scores side-by-side across models\n",
    "- Calculate statistical measures (mean, standard deviation)\n",
    "- Analyze cost-performance trade-offs\n",
    "- Generate actionable model selection recommendations\n",
    "\n",
    "### Tutorial Details\n",
    "\n",
    "| Information         | Details                                                                       |\n",
    "|:--------------------|:------------------------------------------------------------------------------|\n",
    "| Tutorial type       | Advanced - Systematic model evaluation and comparison                         |\n",
    "| Tutorial components | Multi-model evaluation, statistical analysis, cost-performance optimization   |\n",
    "| Tutorial vertical   | Agent Evaluation                                                              |\n",
    "| Example complexity  | Advanced                                                                      |\n",
    "| SDK used            | Strands Agents, Strands Evals                                                 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding A/B Testing Evaluation\n",
    "\n",
    "Systematic evaluation answers: Which model produces highest quality responses? Best completes goals? Selects tools most accurately? Provides best cost-performance ratio?\n",
    "\n",
    "#### Five Dimensions of Evaluation\n",
    "\n",
    "| Evaluator | Purpose |\n",
    "|:----------|:--------|\n",
    "| OutputEvaluator | Measures response quality, correctness, completeness |\n",
    "| HelpfulnessEvaluator | Assesses practical value, clarity, actionability |\n",
    "| GoalSuccessRateEvaluator | Tracks binary success/failure for task completion |\n",
    "| ToolSelectionAccuracyEvaluator | Validates appropriate tool choices |\n",
    "| ToolParameterAccuracyEvaluator | Checks parameter correctness and formatting |\n",
    "\n",
    "#### Evaluation Methodology\n",
    "\n",
    "| Requirement | Description |\n",
    "|:------------|:------------|\n",
    "| Same test dataset | All models tested on identical queries |\n",
    "| Same evaluation criteria | Consistent scoring across variants |\n",
    "| Multiple test cases | Statistical validity through volume |\n",
    "\n",
    "**Analysis Flow**: Run agents \u2192 Collect scores \u2192 Calculate statistics \u2192 Compare side-by-side \u2192 Factor cost \u2192 Generate recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Setup\n",
    "\n",
    "Configure AWS region and prepare for multi-model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# AWS Configuration\n",
    "session = boto3.Session()\n",
    "AWS_REGION = session.region_name or 'us-east-1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup and Imports\n",
    "\n",
    "Import all necessary libraries for evaluation and statistical analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import logging\n",
    "from typing import Dict, List, Any\n",
    "import statistics\n",
    "from collections import defaultdict\n",
    "\n",
    "# Add paths for airline tools (local data directory)\n",
    "sys.path.append('./data/ma-bench/')\n",
    "sys.path.append('./data/tau-bench/')\n",
    "\n",
    "# Strands imports\n",
    "from strands import Agent\n",
    "from strands.models import BedrockModel\n",
    "\n",
    "# Strands Evals imports\n",
    "from strands_evals import Experiment, Case\n",
    "from strands_evals.evaluators import (\n",
    "    OutputEvaluator,\n",
    "    HelpfulnessEvaluator,\n",
    "    GoalSuccessRateEvaluator,\n",
    "    ToolSelectionAccuracyEvaluator,\n",
    "    ToolParameterAccuracyEvaluator\n",
    ")\n",
    "\n",
    "# Display utilities\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Disable verbose logging\n",
    "logging.basicConfig(level=logging.CRITICAL)\n",
    "for logger_name in [\"strands\", \"graph\", \"event_loop\", \"registry\", \"sliding_window_conversation_manager\", \"bedrock\", \"streaming\"]:\n",
    "    logging.getLogger(logger_name).setLevel(logging.CRITICAL)\n",
    "\n",
    "# Bypass tool consent\n",
    "os.environ[\"BYPASS_TOOL_CONSENT\"] = \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Agent Configurations\n",
    "\n",
    "Load the agent configurations saved in Part 1 (Tutorial 07a)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration from Part 1\n",
    "config_path = \"./agent_configs.json\"\n",
    "\n",
    "with open(config_path, \"r\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "print(\"Loaded agent configurations:\")\n",
    "print(f\"  Models: {len(config['models'])}\")\n",
    "print(f\"  Dataset path: {config['dataset_path']}\")\n",
    "print(f\"  Tools: {config['num_tools']}\")\n",
    "print(\"\\nModel variants:\")\n",
    "for key, model_info in config['models'].items():\n",
    "    print(f\"  {model_info['name']}: {model_info['model_id']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Airline Domain Tools\n",
    "\n",
    "Recreate the airline tool environment for all three agent variants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all 14 airline tools\n",
    "from mabench.environments.airline.tools.book_reservation import book_reservation\n",
    "from mabench.environments.airline.tools.calculate import calculate\n",
    "from mabench.environments.airline.tools.cancel_reservation import cancel_reservation\n",
    "from mabench.environments.airline.tools.get_reservation_details import get_reservation_details\n",
    "from mabench.environments.airline.tools.get_user_details import get_user_details\n",
    "from mabench.environments.airline.tools.list_all_airports import list_all_airports\n",
    "from mabench.environments.airline.tools.search_direct_flight import search_direct_flight\n",
    "from mabench.environments.airline.tools.search_onestop_flight import search_onestop_flight\n",
    "from mabench.environments.airline.tools.send_certificate import send_certificate\n",
    "from mabench.environments.airline.tools.think import think\n",
    "from mabench.environments.airline.tools.transfer_to_human_agents import transfer_to_human_agents\n",
    "from mabench.environments.airline.tools.update_reservation_baggages import update_reservation_baggages\n",
    "from mabench.environments.airline.tools.update_reservation_flights import update_reservation_flights\n",
    "from mabench.environments.airline.tools.update_reservation_passengers import update_reservation_passengers\n",
    "\n",
    "# Import airline policy\n",
    "from tau_bench.envs.airline.wiki import WIKI\n",
    "\n",
    "# Define tools list\n",
    "AIRLINE_TOOLS = [\n",
    "    book_reservation,\n",
    "    calculate,\n",
    "    cancel_reservation,\n",
    "    get_reservation_details,\n",
    "    get_user_details,\n",
    "    list_all_airports,\n",
    "    search_direct_flight,\n",
    "    search_onestop_flight,\n",
    "    send_certificate,\n",
    "    think,\n",
    "    transfer_to_human_agents,\n",
    "    update_reservation_baggages,\n",
    "    update_reservation_flights,\n",
    "    update_reservation_passengers,\n",
    "]\n",
    "\n",
    "# Create system prompt\n",
    "SYSTEM_PROMPT_TEMPLATE = \"\"\"\n",
    "You are a helpful assistant for a travel website. Help the user answer any questions.\n",
    "\n",
    "<instructions>\n",
    "- Remember to check if the airport city is in the state mentioned by the user. For example, Houston is in Texas.\n",
    "- Infer about the U.S. state in which the airport city resides. For example, Houston is in Texas.\n",
    "- You should not use made-up or placeholder arguments.\n",
    "</instructions>\n",
    "\n",
    "<policy>\n",
    "{policy}\n",
    "</policy>\n",
    "\"\"\"\n",
    "\n",
    "AIRLINE_SYSTEM_PROMPT = SYSTEM_PROMPT_TEMPLATE.replace(\"{policy}\", WIKI)\n",
    "\n",
    "print(f\"Imported {len(AIRLINE_TOOLS)} airline tools\")\n",
    "print(f\"System prompt created ({len(AIRLINE_SYSTEM_PROMPT)} characters)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recreate All Three Agent Variants\n",
    "\n",
    "Build the three agent variants (Haiku, Sonnet, Nova Lite) for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create BedrockModels for each variant\n",
    "model_haiku = BedrockModel(\n",
    "    region_name=AWS_REGION,\n",
    "    model_id=config['models']['haiku']['model_id']\n",
    ")\n",
    "\n",
    "model_sonnet = BedrockModel(\n",
    "    region_name=AWS_REGION,\n",
    "    model_id=config['models']['sonnet']['model_id']\n",
    ")\n",
    "\n",
    "model_nova_lite = BedrockModel(\n",
    "    region_name=AWS_REGION,\n",
    "    model_id=config['models']['nova_lite']['model_id']\n",
    ")\n",
    "\n",
    "# Create agents\n",
    "agent_haiku = Agent(\n",
    "    name=\"airline_assistant_haiku\",\n",
    "    model=model_haiku,\n",
    "    tools=AIRLINE_TOOLS,\n",
    "    system_prompt=AIRLINE_SYSTEM_PROMPT\n",
    ")\n",
    "\n",
    "agent_sonnet = Agent(\n",
    "    name=\"airline_assistant_sonnet\",\n",
    "    model=model_sonnet,\n",
    "    tools=AIRLINE_TOOLS,\n",
    "    system_prompt=AIRLINE_SYSTEM_PROMPT\n",
    ")\n",
    "\n",
    "agent_nova_lite = Agent(\n",
    "    name=\"airline_assistant_nova_lite\",\n",
    "    model=model_nova_lite,\n",
    "    tools=AIRLINE_TOOLS,\n",
    "    system_prompt=AIRLINE_SYSTEM_PROMPT\n",
    ")\n",
    "\n",
    "# Store in dictionary for easy iteration\n",
    "agents = {\n",
    "    \"haiku\": {\"agent\": agent_haiku, \"name\": \"Claude Haiku\"},\n",
    "    \"sonnet\": {\"agent\": agent_sonnet, \"name\": \"Claude Sonnet\"},\n",
    "    \"nova_lite\": {\"agent\": agent_nova_lite, \"name\": \"Nova Lite\"}\n",
    "}\n",
    "\n",
    "print(\"All three agent variants created:\")\n",
    "for key, info in agents.items():\n",
    "    print(f\"  {info['name']}: Ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load TauBench Dataset\n",
    "\n",
    "Load the full TauBench airline dataset and select diverse test cases for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TauBench dataset\n",
    "dataset_path = config['dataset_path']\n",
    "\n",
    "with open(dataset_path, \"r\") as file:\n",
    "    all_tasks = json.load(file)\n",
    "\n",
    "print(f\"Loaded {len(all_tasks)} total scenarios from TauBench\")\n",
    "print(\"\\nDataset includes:\")\n",
    "print(\"  - Flight searches and bookings\")\n",
    "print(\"  - Reservation modifications\")\n",
    "print(\"  - Cancellations and refunds\")\n",
    "print(\"  - Customer service inquiries\")\n",
    "print(\"  - Policy-based decisions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Diverse Test Cases\n",
    "\n",
    "Choose 12 diverse test cases that represent different complexity levels and scenario types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select diverse test cases (indices chosen for variety)\n",
    "selected_indices = [0, 1] #, 2, 4, 10, 15, 20, 25, 30, 35, 40, 45]\n",
    "selected_tasks = [all_tasks[i] for i in selected_indices]\n",
    "\n",
    "print(f\"Selected {len(selected_tasks)} diverse test cases:\")\n",
    "print(\"\\nTest Case Overview:\")\n",
    "for i, task in enumerate(selected_tasks, 1):\n",
    "    question_preview = task['question'].replace('\\n', ' ') + \"...\"\n",
    "    print(f\"  {i}. {question_preview}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Evaluation Dataset\n",
    "\n",
    "Build Strands Evals dataset with test cases and five evaluators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Case objects for evaluation\n",
    "test_cases = []\n",
    "for i, task in enumerate(selected_tasks):\n",
    "    # Use 'question' if available, otherwise fall back to 'instruction'\n",
    "    user_query = task.get('question', task.get('instruction', ''))\n",
    "    case = Case(\n",
    "        name=f\"Scenario_{i+1}_{task['user_id']}\",\n",
    "        input=user_query,  # \u2705 Works for both schemas\n",
    "        expected_output=\"A helpful, accurate response that addresses the user's airline service request.\"\n",
    "    )\n",
    "    test_cases.append(case)\n",
    "\n",
    "# Create separate experiments for each evaluator\n",
    "evaluator_list = [\n",
    "    (\"OutputEvaluator\", OutputEvaluator(rubric=\"The output should provide a helpful, accurate response to the airline service request.\")),\n",
    "    (\"HelpfulnessEvaluator\", HelpfulnessEvaluator()),\n",
    "    (\"GoalSuccessRateEvaluator\", GoalSuccessRateEvaluator()),\n",
    "    (\"ToolSelectionAccuracyEvaluator\", ToolSelectionAccuracyEvaluator()),\n",
    "    (\"ToolParameterAccuracyEvaluator\", ToolParameterAccuracyEvaluator())\n",
    "]\n",
    "\n",
    "dataset_output = Experiment(\n",
    "    cases=test_cases,\n",
    "    evaluator=evaluator_list[0][1]\n",
    ")\n",
    "\n",
    "dataset_helpfulness = Experiment(\n",
    "    cases=test_cases,\n",
    "    evaluator=evaluator_list[1][1]\n",
    ")\n",
    "\n",
    "dataset_goal = Experiment(\n",
    "    cases=test_cases,\n",
    "    evaluator=evaluator_list[2][1]\n",
    ")\n",
    "\n",
    "dataset_tool_selection = Experiment(\n",
    "    cases=test_cases,\n",
    "    evaluator=evaluator_list[3][1]\n",
    ")\n",
    "\n",
    "dataset_tool_parameter = Experiment(\n",
    "    cases=test_cases,\n",
    "    evaluator=evaluator_list[4][1]\n",
    ")\n",
    "\n",
    "print(f\"Evaluation experiments created:\")\n",
    "print(f\"  Test cases: {len(test_cases)}\")\n",
    "print(f\"  Evaluators: {len(evaluator_list)} (one experiment per evaluator)\")\n",
    "print(\"\\nEvaluator types:\")\n",
    "for evaluator_name, _ in evaluator_list:\n",
    "    print(f\"  - {evaluator_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Evaluation: Claude Haiku\n",
    "\n",
    "Evaluate the Claude Haiku agent variant on all test cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define agent task function for Haiku\n",
    "def agent_task_haiku(case: Case) -> str:\n",
    "    \"\"\"Execute Haiku agent with the given case input.\"\"\"\n",
    "    agent_haiku.messages = []\n",
    "    response = agent_haiku(case.input)\n",
    "    return str(response)\n",
    "\n",
    "# Run evaluations for all 5 evaluators\n",
    "print(\"[EVALUATING HAIKU]\")\n",
    "print(\"Running evaluation on Claude Haiku agent...\")\n",
    "\n",
    "print(\"  [1/5] OutputEvaluator...\")\n",
    "report_haiku_output = dataset_output.run_evaluations(agent_task_haiku)\n",
    "\n",
    "print(\"  [2/5] HelpfulnessEvaluator...\")\n",
    "report_haiku_helpfulness = dataset_helpfulness.run_evaluations(agent_task_haiku)\n",
    "\n",
    "print(\"  [3/5] GoalSuccessRateEvaluator...\")\n",
    "report_haiku_goal = dataset_goal.run_evaluations(agent_task_haiku)\n",
    "\n",
    "print(\"  [4/5] ToolSelectionAccuracyEvaluator...\")\n",
    "report_haiku_tool_selection = dataset_tool_selection.run_evaluations(agent_task_haiku)\n",
    "\n",
    "print(\"  [5/5] ToolParameterAccuracyEvaluator...\")\n",
    "report_haiku_tool_parameter = dataset_tool_parameter.run_evaluations(agent_task_haiku)\n",
    "\n",
    "print(\"Haiku evaluation complete (5 evaluation rounds)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Haiku Results\n",
    "\n",
    "Display evaluation results for Claude Haiku."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CLAUDE HAIKU RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n[1/5] OUTPUT QUALITY\")\n",
    "print(\"-\" * 80)\n",
    "report_haiku_output.display()\n",
    "\n",
    "print(\"\\n[2/5] HELPFULNESS\")\n",
    "print(\"-\" * 80)\n",
    "report_haiku_helpfulness.display()\n",
    "\n",
    "print(\"\\n[3/5] GOAL SUCCESS RATE\")\n",
    "print(\"-\" * 80)\n",
    "report_haiku_goal.display()\n",
    "\n",
    "print(\"\\n[4/5] TOOL SELECTION ACCURACY\")\n",
    "print(\"-\" * 80)\n",
    "report_haiku_tool_selection.display()\n",
    "\n",
    "print(\"\\n[5/5] TOOL PARAMETER ACCURACY\")\n",
    "print(\"-\" * 80)\n",
    "report_haiku_tool_parameter.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Evaluation: Claude Sonnet\n",
    "\n",
    "Evaluate the Claude Sonnet agent variant on all test cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define agent task function for Sonnet\n",
    "def agent_task_sonnet(case: Case) -> str:\n",
    "    \"\"\"Execute Sonnet agent with the given case input.\"\"\"\n",
    "    agent_sonnet.messages = []\n",
    "    response = agent_sonnet(case.input)\n",
    "    return str(response)\n",
    "\n",
    "# Run evaluations for all 5 evaluators\n",
    "print(\"[EVALUATING SONNET]\")\n",
    "print(\"Running evaluation on Claude Sonnet agent...\")\n",
    "\n",
    "print(\"  [1/5] OutputEvaluator...\")\n",
    "report_sonnet_output = dataset_output.run_evaluations(agent_task_sonnet)\n",
    "\n",
    "print(\"  [2/5] HelpfulnessEvaluator...\")\n",
    "report_sonnet_helpfulness = dataset_helpfulness.run_evaluations(agent_task_sonnet)\n",
    "\n",
    "print(\"  [3/5] GoalSuccessRateEvaluator...\")\n",
    "report_sonnet_goal = dataset_goal.run_evaluations(agent_task_sonnet)\n",
    "\n",
    "print(\"  [4/5] ToolSelectionAccuracyEvaluator...\")\n",
    "report_sonnet_tool_selection = dataset_tool_selection.run_evaluations(agent_task_sonnet)\n",
    "\n",
    "print(\"  [5/5] ToolParameterAccuracyEvaluator...\")\n",
    "report_sonnet_tool_parameter = dataset_tool_parameter.run_evaluations(agent_task_sonnet)\n",
    "\n",
    "print(\"Sonnet evaluation complete (5 evaluation rounds)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sonnet Results\n",
    "\n",
    "Display evaluation results for Claude Sonnet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CLAUDE SONNET RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n[1/5] OUTPUT QUALITY\")\n",
    "print(\"-\" * 80)\n",
    "report_sonnet_output.display()\n",
    "\n",
    "print(\"\\n[2/5] HELPFULNESS\")\n",
    "print(\"-\" * 80)\n",
    "report_sonnet_helpfulness.display()\n",
    "\n",
    "print(\"\\n[3/5] GOAL SUCCESS RATE\")\n",
    "print(\"-\" * 80)\n",
    "report_sonnet_goal.display()\n",
    "\n",
    "print(\"\\n[4/5] TOOL SELECTION ACCURACY\")\n",
    "print(\"-\" * 80)\n",
    "report_sonnet_tool_selection.display()\n",
    "\n",
    "print(\"\\n[5/5] TOOL PARAMETER ACCURACY\")\n",
    "print(\"-\" * 80)\n",
    "report_sonnet_tool_parameter.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Evaluation: Nova Lite\n",
    "\n",
    "Evaluate the Nova Lite agent variant on all test cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define agent task function for Nova Lite\n",
    "def agent_task_nova(case: Case) -> str:\n",
    "    \"\"\"Execute Nova Lite agent with the given case input.\"\"\"\n",
    "    agent_nova_lite.messages = []\n",
    "    response = agent_nova_lite(case.input)\n",
    "    return str(response)\n",
    "\n",
    "# Run evaluations for all 5 evaluators\n",
    "print(\"[EVALUATING NOVA LITE]\")\n",
    "print(\"Running evaluation on Nova Lite agent...\")\n",
    "\n",
    "print(\"  [1/5] OutputEvaluator...\")\n",
    "report_nova_output = dataset_output.run_evaluations(agent_task_nova)\n",
    "\n",
    "print(\"  [2/5] HelpfulnessEvaluator...\")\n",
    "report_nova_helpfulness = dataset_helpfulness.run_evaluations(agent_task_nova)\n",
    "\n",
    "print(\"  [3/5] GoalSuccessRateEvaluator...\")\n",
    "report_nova_goal = dataset_goal.run_evaluations(agent_task_nova)\n",
    "\n",
    "print(\"  [4/5] ToolSelectionAccuracyEvaluator...\")\n",
    "report_nova_tool_selection = dataset_tool_selection.run_evaluations(agent_task_nova)\n",
    "\n",
    "print(\"  [5/5] ToolParameterAccuracyEvaluator...\")\n",
    "report_nova_tool_parameter = dataset_tool_parameter.run_evaluations(agent_task_nova)\n",
    "\n",
    "print(\"Nova Lite evaluation complete (5 evaluation rounds)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nova Lite Results\n",
    "\n",
    "Display evaluation results for Nova Lite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"NOVA LITE RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n[1/5] OUTPUT QUALITY\")\n",
    "print(\"-\" * 80)\n",
    "report_nova_output.display()\n",
    "\n",
    "print(\"\\n[2/5] HELPFULNESS\")\n",
    "print(\"-\" * 80)\n",
    "report_nova_helpfulness.display()\n",
    "\n",
    "print(\"\\n[3/5] GOAL SUCCESS RATE\")\n",
    "print(\"-\" * 80)\n",
    "report_nova_goal.display()\n",
    "\n",
    "print(\"\\n[4/5] TOOL SELECTION ACCURACY\")\n",
    "print(\"-\" * 80)\n",
    "report_nova_tool_selection.display()\n",
    "\n",
    "print(\"\\n[5/5] TOOL PARAMETER ACCURACY\")\n",
    "print(\"-\" * 80)\n",
    "report_nova_tool_parameter.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG: Inspect report structure to understand how to extract scores\n",
    "print(\"=\" * 80)\n",
    "print(\"DEBUG: Inspecting report_haiku_output structure\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n1. Report type:\")\n",
    "print(f\"   {type(report_haiku_output)}\")\n",
    "\n",
    "print(\"\\n2. Report attributes:\")\n",
    "report_attrs = [attr for attr in dir(report_haiku_output) if not attr.startswith('_')]\n",
    "print(f\"   {report_attrs}\")\n",
    "\n",
    "print(\"\\n3. Checking for 'detailed_results' attribute:\")\n",
    "if hasattr(report_haiku_output, 'detailed_results'):\n",
    "    print(f\"   Type: {type(report_haiku_output.detailed_results)}\")\n",
    "    if report_haiku_output.detailed_results:\n",
    "        print(f\"   Length: {len(report_haiku_output.detailed_results)}\")\n",
    "        print(f\"   First element type: {type(report_haiku_output.detailed_results[0])}\")\n",
    "        print(f\"   First element: {report_haiku_output.detailed_results[0]}\")\n",
    "else:\n",
    "    print(\"   'detailed_results' not found!\")\n",
    "\n",
    "print(\"\\n4. Checking for 'results' attribute:\")\n",
    "if hasattr(report_haiku_output, 'results'):\n",
    "    print(f\"   Type: {type(report_haiku_output.results)}\")\n",
    "    if report_haiku_output.results:\n",
    "        print(f\"   Length: {len(report_haiku_output.results)}\")\n",
    "        print(f\"   First element type: {type(report_haiku_output.results[0])}\")\n",
    "        print(f\"   First element attributes: {[attr for attr in dir(report_haiku_output.results[0]) if not attr.startswith('_')]}\")\n",
    "        print(f\"   First element: {report_haiku_output.results[0]}\")\n",
    "        \n",
    "        # Check if first element has score\n",
    "        if hasattr(report_haiku_output.results[0], 'score'):\n",
    "            print(f\"   First element score: {report_haiku_output.results[0].score}\")\n",
    "else:\n",
    "    print(\"   'results' not found!\")\n",
    "\n",
    "print(\"\\n5. All non-private attributes with values:\")\n",
    "for attr in report_attrs[:10]:  # Show first 10 to avoid too much output\n",
    "    try:\n",
    "        value = getattr(report_haiku_output, attr)\n",
    "        if not callable(value):\n",
    "            print(f\"   {attr}: {type(value)} = {str(value)[:100]}\")\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG: Inspect report structure to understand how to extract scores\n",
    "print(\"=\" * 80)\n",
    "print(\"DEBUG: Inspecting report_haiku_output structure\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n1. Report type:\")\n",
    "print(f\"   {type(report_haiku_output)}\")\n",
    "\n",
    "print(\"\\n2. Report attributes:\")\n",
    "report_attrs = [attr for attr in dir(report_haiku_output) if not attr.startswith('_')]\n",
    "print(f\"   {report_attrs}\")\n",
    "\n",
    "print(\"\\n3. Checking for 'detailed_results' attribute:\")\n",
    "if hasattr(report_haiku_output, 'detailed_results'):\n",
    "    print(f\"   Type: {type(report_haiku_output.detailed_results)}\")\n",
    "    if report_haiku_output.detailed_results:\n",
    "        print(f\"   Length: {len(report_haiku_output.detailed_results)}\")\n",
    "        print(f\"   First element type: {type(report_haiku_output.detailed_results[0])}\")\n",
    "        print(f\"   First element: {report_haiku_output.detailed_results[0]}\")\n",
    "else:\n",
    "    print(\"   'detailed_results' not found!\")\n",
    "\n",
    "print(\"\\n4. Checking for 'results' attribute:\")\n",
    "if hasattr(report_haiku_output, 'results'):\n",
    "    print(f\"   Type: {type(report_haiku_output.results)}\")\n",
    "    if report_haiku_output.results:\n",
    "        print(f\"   Length: {len(report_haiku_output.results)}\")\n",
    "        print(f\"   First element type: {type(report_haiku_output.results[0])}\")\n",
    "        print(f\"   First element attributes: {[attr for attr in dir(report_haiku_output.results[0]) if not attr.startswith('_')]}\")\n",
    "        print(f\"   First element: {report_haiku_output.results[0]}\")\n",
    "        \n",
    "        # Check if first element has score\n",
    "        if hasattr(report_haiku_output.results[0], 'score'):\n",
    "            print(f\"   First element score: {report_haiku_output.results[0].score}\")\n",
    "else:\n",
    "    print(\"   'results' not found!\")\n",
    "\n",
    "print(\"\\n5. All non-private attributes with values:\")\n",
    "for attr in report_attrs[:10]:  # Show first 10 to avoid too much output\n",
    "    try:\n",
    "        value = getattr(report_haiku_output, attr)\n",
    "        if not callable(value):\n",
    "            print(f\"   {attr}: {type(value)} = {str(value)[:100]}\")\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to extract scores from report\n",
    "def extract_scores(report, evaluator_name: str) -> Dict[str, List[float]]:\n",
    "    \"\"\"Extract numeric scores organized by evaluator type.\n",
    "    \n",
    "    Args:\n",
    "        report: EvaluationReport object containing detailed_results\n",
    "        evaluator_name: Name of the evaluator (e.g., 'OutputEvaluator', 'HelpfulnessEvaluator')\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary mapping evaluator name to list of scores\n",
    "    \"\"\"\n",
    "    scores = defaultdict(list)\n",
    "    \n",
    "    # detailed_results is a list of lists of EvaluationOutput objects\n",
    "    # Each test case has its own list of EvaluationOutput objects\n",
    "    for test_case_results in report.detailed_results:\n",
    "        # test_case_results is a list of EvaluationOutput objects for one test case\n",
    "        for eval_output in test_case_results:\n",
    "            # eval_output is an EvaluationOutput object with fields: score, test_pass, reason, label\n",
    "            if hasattr(eval_output, 'score'):\n",
    "                scores[evaluator_name].append(float(eval_output.score))\n",
    "    \n",
    "    return dict(scores)\n",
    "\n",
    "# Extract scores from all evaluation reports (3 models x evaluators)\n",
    "scores_haiku = {}\n",
    "scores_haiku.update(extract_scores(report_haiku_output, 'OutputEvaluator'))\n",
    "scores_haiku.update(extract_scores(report_haiku_helpfulness, 'HelpfulnessEvaluator'))\n",
    "scores_haiku.update(extract_scores(report_haiku_goal, 'GoalSuccessRateEvaluator'))\n",
    "scores_haiku.update(extract_scores(report_haiku_tool_selection, 'ToolSelectionAccuracyEvaluator'))\n",
    "scores_haiku.update(extract_scores(report_haiku_tool_parameter, 'ToolParameterAccuracyEvaluator'))\n",
    "\n",
    "scores_sonnet = {}\n",
    "scores_sonnet.update(extract_scores(report_sonnet_output, 'OutputEvaluator'))\n",
    "scores_sonnet.update(extract_scores(report_sonnet_helpfulness, 'HelpfulnessEvaluator'))\n",
    "scores_sonnet.update(extract_scores(report_sonnet_goal, 'GoalSuccessRateEvaluator'))\n",
    "scores_sonnet.update(extract_scores(report_sonnet_tool_selection, 'ToolSelectionAccuracyEvaluator'))\n",
    "scores_sonnet.update(extract_scores(report_sonnet_tool_parameter, 'ToolParameterAccuracyEvaluator'))\n",
    "\n",
    "scores_nova = {}\n",
    "scores_nova.update(extract_scores(report_nova_output, 'OutputEvaluator'))\n",
    "scores_nova.update(extract_scores(report_nova_helpfulness, 'HelpfulnessEvaluator'))\n",
    "scores_nova.update(extract_scores(report_nova_goal, 'GoalSuccessRateEvaluator'))\n",
    "scores_nova.update(extract_scores(report_nova_tool_selection, 'ToolSelectionAccuracyEvaluator'))\n",
    "scores_nova.update(extract_scores(report_nova_tool_parameter, 'ToolParameterAccuracyEvaluator'))\n",
    "\n",
    "# Calculate totals dynamically\n",
    "num_evaluators = len(scores_haiku)\n",
    "num_models = 3\n",
    "total_reports = num_models * num_evaluators\n",
    "\n",
    "print(f\"Scores extracted from all {total_reports} evaluation reports\")\n",
    "print(f\"  Haiku: {num_evaluators} evaluation rounds\")\n",
    "print(f\"  Sonnet: {num_evaluators} evaluation rounds\")\n",
    "print(f\"  Nova: {num_evaluators} evaluation rounds\")\n",
    "print(f\"\\nEvaluator types found:\")\n",
    "for evaluator_name in scores_haiku.keys():\n",
    "    print(f\"  - {evaluator_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Statistical Measures\n",
    "\n",
    "Compute mean and standard deviation for each evaluator across all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate statistics for each model\n",
    "def calculate_statistics(scores: Dict[str, List[float]]) -> Dict[str, Dict[str, float]]:\n",
    "    \"\"\"Calculate mean and standard deviation for each evaluator.\"\"\"\n",
    "    stats = {}\n",
    "    \n",
    "    for evaluator_name, score_list in scores.items():\n",
    "        if len(score_list) > 0:\n",
    "            mean_score = statistics.mean(score_list)\n",
    "            std_dev = statistics.stdev(score_list) if len(score_list) > 1 else 0.0\n",
    "            stats[evaluator_name] = {\n",
    "                \"mean\": mean_score,\n",
    "                \"std_dev\": std_dev,\n",
    "                \"count\": len(score_list)\n",
    "            }\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# Calculate for all models\n",
    "stats_haiku = calculate_statistics(scores_haiku)\n",
    "stats_sonnet = calculate_statistics(scores_sonnet)\n",
    "stats_nova = calculate_statistics(scores_nova)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Side-by-Side Comparison\n",
    "\n",
    "Display evaluation scores side-by-side for direct comparison across models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\n",
    "comparison_md = \"## Model Performance Comparison\\n\\n\"\n",
    "comparison_md += \"### Mean Scores by Evaluator\\n\\n\"\n",
    "comparison_md += \"| Evaluator | Claude Haiku | Claude Sonnet | Nova Lite |\\n\"\n",
    "comparison_md += \"|:----------|-------------:|--------------:|----------:|\\n\"\n",
    "\n",
    "# Get all evaluator names\n",
    "all_evaluators = set(stats_haiku.keys()) | set(stats_sonnet.keys()) | set(stats_nova.keys())\n",
    "\n",
    "for evaluator_name in sorted(all_evaluators):\n",
    "    haiku_mean = stats_haiku.get(evaluator_name, {}).get('mean', 0.0)\n",
    "    sonnet_mean = stats_sonnet.get(evaluator_name, {}).get('mean', 0.0)\n",
    "    nova_mean = stats_nova.get(evaluator_name, {}).get('mean', 0.0)\n",
    "    \n",
    "    # Highlight best score\n",
    "    max_score = max(haiku_mean, sonnet_mean, nova_mean)\n",
    "    \n",
    "    haiku_str = f\"**{haiku_mean:.3f}**\" if haiku_mean == max_score else f\"{haiku_mean:.3f}\"\n",
    "    sonnet_str = f\"**{sonnet_mean:.3f}**\" if sonnet_mean == max_score else f\"{sonnet_mean:.3f}\"\n",
    "    nova_str = f\"**{nova_mean:.3f}**\" if nova_mean == max_score else f\"{nova_mean:.3f}\"\n",
    "    \n",
    "    comparison_md += f\"| {evaluator_name} | {haiku_str} | {sonnet_str} | {nova_str} |\\n\"\n",
    "\n",
    "comparison_md += \"\\n*Bold indicates best performance for that evaluator*\\n\"\n",
    "\n",
    "display(Markdown(comparison_md))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Variability Analysis\n",
    "\n",
    "Examine the standard deviation to understand consistency across test cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create variability table\n",
    "variability_md = \"## Performance Variability (Standard Deviation)\\n\\n\"\n",
    "variability_md += \"| Evaluator | Claude Haiku | Claude Sonnet | Nova Lite |\\n\"\n",
    "variability_md += \"|:----------|-------------:|--------------:|----------:|\\n\"\n",
    "\n",
    "for evaluator_name in sorted(all_evaluators):\n",
    "    haiku_std = stats_haiku.get(evaluator_name, {}).get('std_dev', 0.0)\n",
    "    sonnet_std = stats_sonnet.get(evaluator_name, {}).get('std_dev', 0.0)\n",
    "    nova_std = stats_nova.get(evaluator_name, {}).get('std_dev', 0.0)\n",
    "    \n",
    "    # Highlight lowest variability (most consistent)\n",
    "    min_std = min(haiku_std, sonnet_std, nova_std) if any([haiku_std, sonnet_std, nova_std]) else 0\n",
    "    \n",
    "    haiku_str = f\"**{haiku_std:.3f}**\" if haiku_std == min_std and min_std > 0 else f\"{haiku_std:.3f}\"\n",
    "    sonnet_str = f\"**{sonnet_std:.3f}**\" if sonnet_std == min_std and min_std > 0 else f\"{sonnet_std:.3f}\"\n",
    "    nova_str = f\"**{nova_std:.3f}**\" if nova_std == min_std and min_std > 0 else f\"{nova_std:.3f}\"\n",
    "    \n",
    "    variability_md += f\"| {evaluator_name} | {haiku_str} | {sonnet_str} | {nova_str} |\\n\"\n",
    "\n",
    "variability_md += \"\\n*Bold indicates lowest variability (most consistent performance)*\\n\"\n",
    "variability_md += \"\\n**Lower standard deviation = More consistent performance across test cases**\\n\"\n",
    "\n",
    "display(Markdown(variability_md))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost-Performance Analysis\n",
    "\n",
    "Analyze cost-performance trade-offs based on AWS Bedrock pricing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approximate pricing (as of 2024, per 1K tokens)\n",
    "# Note: Actual pricing may vary by region and over time\n",
    "pricing = {\n",
    "    \"haiku\": {\"input\": 0.00025, \"output\": 0.00125},  # Haiku pricing\n",
    "    \"sonnet\": {\"input\": 0.003, \"output\": 0.015},    # Sonnet pricing\n",
    "    \"nova_lite\": {\"input\": 0.00006, \"output\": 0.00024}  # Nova Lite pricing\n",
    "}\n",
    "\n",
    "# Create cost-performance summary\n",
    "cost_md = \"## Cost-Performance Trade-offs\\n\\n\"\n",
    "cost_md += \"### Approximate Pricing (per 1K tokens)\\n\\n\"\n",
    "cost_md += \"| Model | Input Cost | Output Cost | Relative Cost |\\n\"\n",
    "cost_md += \"|:------|:-----------|:------------|:--------------|\\n\"\n",
    "\n",
    "# Calculate relative costs (normalized to Haiku)\n",
    "haiku_total = pricing[\"haiku\"][\"input\"] + pricing[\"haiku\"][\"output\"]\n",
    "sonnet_total = pricing[\"sonnet\"][\"input\"] + pricing[\"sonnet\"][\"output\"]\n",
    "nova_total = pricing[\"nova_lite\"][\"input\"] + pricing[\"nova_lite\"][\"output\"]\n",
    "\n",
    "cost_md += f\"| Claude Haiku | ${pricing['haiku']['input']:.5f} | ${pricing['haiku']['output']:.5f} | 1.0x (baseline) |\\n\"\n",
    "cost_md += f\"| Claude Sonnet | ${pricing['sonnet']['input']:.5f} | ${pricing['sonnet']['output']:.5f} | {sonnet_total/haiku_total:.1f}x |\\n\"\n",
    "cost_md += f\"| Nova Lite | ${pricing['nova_lite']['input']:.6f} | ${pricing['nova_lite']['output']:.6f} | {nova_total/haiku_total:.2f}x |\\n\"\n",
    "\n",
    "cost_md += \"\\n*Note: Pricing is approximate and may vary by region*\\n\"\n",
    "\n",
    "display(Markdown(cost_md))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Selection Recommendations\n",
    "\n",
    "Generate actionable recommendations based on evaluation results and cost analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate overall performance score (simple average of means)\n",
    "def overall_score(stats: Dict[str, Dict[str, float]]) -> float:\n",
    "    \"\"\"Calculate simple average of all mean scores.\"\"\"\n",
    "    means = [stat['mean'] for stat in stats.values()]\n",
    "    return statistics.mean(means) if means else 0.0\n",
    "\n",
    "overall_haiku = overall_score(stats_haiku)\n",
    "overall_sonnet = overall_score(stats_sonnet)\n",
    "overall_nova = overall_score(stats_nova)\n",
    "\n",
    "# Create recommendations\n",
    "recommendations_md = \"## Model Selection Recommendations\\n\\n\"\n",
    "recommendations_md += \"### Overall Performance Scores\\n\\n\"\n",
    "recommendations_md += f\"- **Claude Haiku**: {overall_haiku:.3f}\\n\"\n",
    "recommendations_md += f\"- **Claude Sonnet**: {overall_sonnet:.3f}\\n\"\n",
    "recommendations_md += f\"- **Nova Lite**: {overall_nova:.3f}\\n\"\n",
    "recommendations_md += \"\\n### Use Case Recommendations\\n\\n\"\n",
    "\n",
    "# Determine winner\n",
    "best_model = max([(\"Haiku\", overall_haiku), (\"Sonnet\", overall_sonnet), (\"Nova Lite\", overall_nova)], key=lambda x: x[1])\n",
    "\n",
    "recommendations_md += f\"**Best Overall Performance**: Claude {best_model[0]}\\n\\n\"\n",
    "display(Markdown(recommendations_md))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "You've successfully learned how to perform systematic A/B testing evaluation for agent models. You now understand:\n",
    "\n",
    "- How to load agent configurations and recreate multiple variants\n",
    "- Creating diverse evaluation datasets from real-world scenarios\n",
    "- Running systematic evaluations with multiple built-in evaluators\n",
    "- Extracting and aggregating evaluation scores across models\n",
    "- Calculating statistical measures (mean, standard deviation)\n",
    "- Comparing model performance side-by-side\n",
    "- Analyzing cost-performance trade-offs\n",
    "- Generating actionable model selection recommendations\n",
    "\n",
    "This evaluation methodology enables data-driven decision making for model selection in production systems. By combining performance metrics with cost analysis, you can optimize for both quality and efficiency based on your specific requirements."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}